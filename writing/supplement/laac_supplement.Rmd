---
title: "tbd..."
author: "tbd..."
subtitle: Supplementary material
output:
  bookdown::html_document2:
    toc: yes
    toc_float: yes
    fig_caption: yes
    code_folding: hide
    number_sections: no
  bookdown::pdf_document2:
    toc: yes
    number_sections: no
  html_document:
    toc: yes
    df_print: paged
  pdf_document:
    toc: yes
bibliography: ../library.bib
csl: apa6.csl
#header-includes: \usepackage{caption} \renewcommand{\thetable}{S\arabic{table}} \renewcommand{\thefigure}{S\arabic{figure}}
---

```{r, include = F}
knitr::opts_chunk$set(echo=F, warning=FALSE, message=FALSE, size="small")

opts <- options(knitr.kable.NA = "")
```


```{r, cache = F, include = F}
library(png)
library(grid)
library(xtable)
library(tidyverse)
library(tidybayes)
library(ggridges)
library(glue)
library(brms)
library(stringr)
library(forcats)
library(ggthemes)
library(ggpubr)
library(reshape)
library(ggExtra)
library(tidybayes)
library(tidyboot)
library(MplusAutomation)
library(kableExtra)
library(knitr)
#library(magick)
#library(pdftools)
library(projpred)
library(bayesplot)
library(loo)
library(directlabels)

source("../../utils/custom_facet.R")

cor_func <- function(x) {
  x %>%
    corrr::correlate()%>%
    gather(time_point, cor, -term)%>%
    mutate(cor = replace(cor, duplicated(cor), NA))%>%
    drop_na(cor)
}

library(coda)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}

hdi_upper<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}

hdi_lower<- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
```

```{r}
# read in data files
data_task <- read.csv("../../data/laac_data_task.csv") 
data_trial <- read.csv("../../data/laac_data_trial.csv") 
```



# Overview

This document gives a detailed overview of the methods used in the study "...tbd...". First, we give an overview of our great ape [participants](#participants). Next we describe the general [setup](#setup) and the experimental [tasks](#tasks) that were used. In the section [data collection](#data-collection) we lay out the time line of data collection. Next, we give an overview of the [predictor variables](#predictors) we recorded in addition to the experimental data. 

We then move on to describe the two parts of our [analytical framework](#analytical-framework): [Structural Equation Modeling](#structural-equation-modeling) to investigate stability and reliability of cognitive performance and [Projection Predictive Inference](#projection-predictive-inference) to test the importance of the predictor variables. 

We present the [results](#results) separate for the two phases of data collection. For each phase, we first report results on [stability and reliability](#stability-and-reliability) of performance within each task and then we investigate [relations between](#relations-between-tasks) performance in the different tasks. Finally, we report how  the different [predictors](#predictability) realted to performance in the different tasks. 

The [appendix](#appendix) contains results from simulation studies we conducted to investigate the performance of the employed Structural Equation Models under the sample sizes given in the present dataset. 

# Methods

## Participants

```{r}
participants <- data_trial%>%
  mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(species)%>%
  mutate(minage = round(min(age),1),
         maxage = round(max(age),1))%>%
  group_by(species, sex, minage,maxage)%>%
  summarise(n = length(unique(subject)))
  
  
 tpn <- data_trial%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
   group_by(time_point)%>%
   mutate(min_date = min(date))%>%
   group_by(time_point,group,.drop=FALSE)%>%
  summarise(n = length(unique(subject)),
            date = min(min_date))%>%
  group_by(time_point)%>%
  mutate(total_n = sum(n),
         date = as.Date(as.character(min(date)), "%Y%m%d"))%>%
   group_by(group)%>%
   mutate(end = date, 
          start = lag(as.character(end)))%>%
   mutate(start = ifelse(is.na(start),"2020-08-01",start),
          start = as.Date(start, format = "%Y-%m-%d"))%>%
   group_by(time_point)%>%
   mutate(shade = time_point %% 2 == 0)
   
```
A total of `r sum(participants$n)` great apes participated at least once in one of the tasks. This included `r participants%>%filter(species == "bonobo")%>%pull(n)%>%sum()` Bonobos (`r participants%>%filter(species == "bonobo", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "bonobo", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "bonobo", sex == "f")%>%pull(maxage)`), `r participants%>%filter(species == "chimpanzee")%>%pull(n)%>%sum()` Chimpanzees (`r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "chimpanzee", sex == "f")%>%pull(maxage)`), `r participants%>%filter(species == "gorilla")%>%pull(n)%>%sum()` Gorillas (`r participants%>%filter(species == "gorilla", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "gorilla", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "gorilla", sex == "f")%>%pull(maxage)`), and `r participants%>%filter(species == "gorilla")%>%pull(n)%>%sum()` Orangutans (`r participants%>%filter(species == "orangutan", sex == "f")%>%pull(n)` females, age `r participants%>%filter(species == "orangutan", sex == "f")%>%pull(minage)` to `r participants%>%filter(species == "orangutan", sex == "f")%>%pull(maxage)`). The sample size at the different time points ranged from `r min(tpn$n)` to `r max(tpn$n)` for the different species. Figure \@ref(fig:sample) visualizes the sample size across time points. We tried to test all apes at all time points but this was not always possible due to a lack of motivation or construction works. All apes participated in cognitive research on a regular basis. Many of them had ample experience with the very tasks we used in the current study.

Apes were housed at the Wolfgang Köhler Primate Research Center located in Zoo Leipzig, Germany. They lived in groups, with one group per species and two chimpanzee groups (group A and B). Research was noninvasive and strictly adhered to the legal requirements in Germany. Animal husbandry and research complied with the European Association of Zoos and Aquaria Minimum Standards for the Accommodation and Care of Animals in Zoos and Aquaria as well as the World Association of Zoos and Aquariums Ethical Guidelines for the Conduct of Research on Animals by Zoos and Aquariums. Participation was voluntary, all food was given in addition to the daily diet, and water was available ad libitum throughout the study. The study was approved by an internal ethics committee at the Max Planck Institute for Evolutionary Anthropology.

```{r sample, out.width="100%", fig.height = 3, fig.cap = "Sample size by species across the different time points. Time point specific predictor variables were collected during the time between two time points (shaded regions) to predict the next."}

cols <- c(ptol_pal()(5),"black")

ggplot(tpn, aes(x = date, y = n, col = group))+
  geom_rect(aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf, fill= shade), col = NA, alpha =.1)+
  geom_rect(aes(xmin = as.Date("2020-08-01"), xmax = as.Date("2021-03-05"), ymin = 0, ymax = 44),fill = NA, col = "darkgrey")+
  geom_rect(aes(xmin = as.Date("2021-05-10"), xmax = max(date), ymin = 0, ymax = 44),fill = NA, col = "darkgrey")+
  annotate(geom="text", x=as.Date("2020-11-14"), y=46, label="Phase 1", color="black", size=4)+
  annotate(geom="text", x=as.Date("2021-09-14"), y=46, label="Phase 2", color="black", size=4)+
  geom_point(alpha = .6)+
  geom_line(alpha = .6)+
  geom_point(aes(y = total_n), col = "black", alpha = .6)+
  geom_line(aes(y = total_n), col = "black", alpha = .6)+
  theme_minimal()+
  ylim(0,48)+
  labs(y = "Sample Size", x = "")+
  scale_color_manual(name = "Species", 
                   limits = c("a_chimp", "b_chimp", "bonobo", "gorilla", "orangutan", "total"),
                   labels =c("a-chimpanzee", "b-chimpanzee", "bonobo", "gorilla", "orangutan", "total"),
                   values = cols)+
  scale_x_date(date_breaks = "1 month",
             date_labels = "%b")+
  scale_fill_manual(values = c("grey", NA))+
  guides(fill = F)
```
## Setup

Apes were tested in familiar sleeping or observation rooms by a single experimenter. Whenever possible, they were tested individually. The basic setup comprised a sliding table positioned in front of a clear Plexiglas panel with three holes in it. The experimenter sat on a small stool and used an occluder to cover the sliding table (see Figure \@ref(fig:setup)).

```{r setup, include = T, fig.cap = "Setup used for the six tasks. A) Gaze following. B) Causal reasoning and inference by exclusion. C) Quantity discrimination. D) Switching. E) Delay of gratification. F) Order of task presentation and trial numbers", out.width="100%"}
knitr::include_graphics("figures/setup.png") 
```

## Tasks

The tasks we selected are based on published procedures and are commonly used in the field of comparative psychology. The original publications often include control conditions to rule out alternative, cognitively less demanding ways to solve the tasks. We did not include such controls here and only ran the experimental conditions. For each task, we refer to the publication we used to model our procedure. We ask the reader to read these papers if they want to know more about control conditions and/or a detailed discussion of the nature of the underlying cognitive mechanisms.

Example videos for each task can be found in the associated online repository in [`videos/`](https://github.com/ccp-eva/laac/tree/master/videos).

### Gaze Following

The gaze following task was modeled after @brauer2005all. The experimenter sat opposite the ape and handed over food at a constant pace. That is, the experimenter picked up a piece of food, briefly held it out in front of her face and then handed it over to the participant. After a predetermined (but varying) number of food items had been handed over, the experimenter again picked up a food item, held it in front of her face and then looked up (i.e., moving her head up - see Figure \@ref(fig:setup)C). The experimenter looked to the ceiling, no object of particular interest was placed there. After 10s, the experimenter looked down again, handed over the food and the trial ended. We coded whether the participant looked up during the 10s interval. Apes received eight gaze following trials.

We assume that participants look up becasue they assume that the experimenter's attention is focused on a potentially noteworthy object. 

### Causal inference

The causal inference task was modeled after @call2004inferences. Two identical cups with a lid were placed left and right on the table (Figure \@ref(fig:setup)A). The experimenter covered the table with the occluder, retrieved a piece of food, showed it to the ape, and hid it in one the cups outside the participant’s view. Next, the experimenter removed the occluder, picked up the baited cup and shook it three times, which produced a rattling sound. Next, the cup was put back in place, the sliding table pushed forwards, and the participant made a choice by pointing to one of the cups. If they picked the baited cup, their choice was coded as correct, and they received the reward. If they chose the empty cup, they did not. Participants received 12 trials. The location of the food was counterbalanced; six times in the right cup and six times in the left. Causal inference trials were intermixed with inference by exclusion trials (see below).

We assume that apes locate the food by reasoning that the food -- a solid object -- causes the rattling sound and therefore must be in the shaken cup.

### Inference by exclusion

Inference by exclusion trials were also modeled after @call2004inferences and followed a very similar procedure compared to causal inference trials. After covering the two cups with the occluder, the experimenter placed the food in one of the cups and covered both with the lid. Next, they removed the occluder, picked up the empty cup and shook it three times. In contrast to the causal inference trials, this did not produce any sound. The experimenter then pushed the sliding table forward and the participant made a choice by pointing to one of the cups. Correct choice was coded when the baited (non-shaken) cup was chosen. If correct, the food was given to the ape. There were 12 inference by exclusion trials, intermixed with causal inference trials. The order was counterbalanced: six times the left cup was baited, six times the right. 

We assume that apes reason that the absence of a sound suggests that the shaken cup is empty. Because they saw a piece of food being hidden, they exclude the empty cup and infer that the food is more likely to be in the non-shaken cup. 

### Quantity discrimination

For this task, we followed the general procedure of @hanus2007discrete. Two small plates were presented left and right on the table (see Figure \@ref(fig:setup)B). The experimenter covered the plates with the occluder and placed five small food pieces on one plate and seven on the other. Then they pushed the sliding table forwards, and the participant made a choice. We coded as correct when the subject chose the plate with the larger quantity. Participants always received the food from the plate they chose. There were 12 trials, six with the larger quantity on the right and six on the left (order counterbalanced).

We assume that apes identify the larger of the two food amounts based on discrete quantity estimation.

### Switching

This task was modeled after @haun2006evolutionary. Three differently looking cups (silver metal cup with handle, green plastic ice cone, red cup without handle - Figure \@ref(fig:setup)D) were placed next to each other on the table. There were two conditions. In the place condition, the experimenter hid a piece of food under one of the cups in full view of the participant. Next, the cups were covered by the occluder and the experimenter switched the position of two cups, while the reward remained in the same location. Next, the experimenter removed the occluder and pushed the table forward. We coded as correct if the participant chose the location where the food was hidden. Participants received four trials in this condition. 

The place condition was run first. The subsequent feature condition followed the same procedure, but now the experimenter also moved the reward when switching the cups. The switch between conditions happened without informing the participant in any way. A correct choice in this condition meant choosing the location to which the cup plus the food were moved. Here, participants received eight trials. 

The dependent measure of interest for this task was calculated as: `[proportion correct place] - (1 - [proportion correct feature])`.  Positive values in this score mean that participants could quickly switch from choosing based on location to choosing based on feature. High negative values suggest that participants did not or hardly switch strategies.  

Based on the results of @haun2006evolutionary, we assume that apes have a tendency to expect the food to remain in the same location. When this strategy is no longer successful in the feature trials, they have to switch strategies and try a different one. 

The switching task was only used in Phase 1. It did not produce meaningful results (see results for Phase 1 in [Stability and Reliability](#stability-and-reliability) below) and for Phase 2 we therefore replaced it with a delay of gratification task (see below).

### Delay of gratification

This task replaced the switching task in Phase 2. The procedure was adapted from @rosati2007evolutionary. Two small plates including one and two pieces of pellet were presented left and right on the table. E moved the plate with the smaller reward forward allowing the subject to choose immediately, while the plate with the larger reward was moved forward after a delay of 20 seconds. We coded whether the subject selected the larger delayed reward (correct choice) or the smaller immediate reward (incorrect choice) as well as the waiting time in cases where the immediate reward was chosen. Subjects received 12 trials, with the side on which the immediate reward was presented counterbalanced.

We assume that, in order to choose the larger reward, apes inhibit choosing the immediate smaller reward.


## Data collection

One time point meant running all tasks with all participants. Within each time point, the tasks were organized in two sessions (see Figure \@ref(fig:setup)F). Session 1 started with two gaze following trials. Next was a pseudo randomized mix of causal inference and inference by exclusion trials with 12 trials per task, but no more than two trials of the same task in a row. At the end of Session 1, there were again two gaze following trials. Session 2 also started with two gaze following trials, followed by quantity discrimination and switching (Phase 1) or Delay of Gratification (Phase 2). Finally, there were again two gaze following trials. By spreading out or mixing tasks we hoped to keep subjects more attentive and engaged. 

The order of tasks was the same for all subjects. So was the positioning of food items within each task. The counterbalancing can be found in the coding sheets in the online repository in `documentation/` [**to be added**]. This exact procedure was repeated at each time point so that the results would be comparable across participants and time points. The two sessions were usually spread out across two adjacent days. For the larger chimpanzee group, they were sometimes spread out across four days. 

The interval between two time points was planned to be two weeks. However, it was not always possible to follow this schedule so that some intervals were longer or shorter. Figure \@ref(fig:sample) visualizes the intervals between time points. 

We collected data in two phases. Phase 1 started on August 1st, 2020, lasted until March 5th, 2021 and included 14 time points. Phase 2 started on May 26th, 2021 and lasted until December 4th, 2021 and also had 14 time points (see Figure \@ref(fig:sample)).

## Predictors

In addition to the data from the cognitive tasks, we collected data for a range of predictor variables. The goal here was to find variables that are systematically related to inter- and/or intra-individual variation in cognitive performance. That is, we were interested to see which variables allow us to predict cognitive performance. The second part of the [analysis](#projection-predictive-inference) section describes the method we used to determine the predictive value of each variable.    

Predictors could either vary with the individual (stable individual characteristics; e.g. sex or rearing history), vary with individual and time point (variable individual characteristics; e.g. sickness or sociality), vary with group membership (group life; e.g. time spent outdoors or disturbances) or vary with the testing arrangements and thus with individual, time point and session (testing arrangements; e.g. presence of an observer or participation in other tests).

Most predictors were collected via a diary that the animal caretakers filled out on a daily basis. Here, the caretakers were asked a range of questions about the presence of a predictor and its severity. The diary (in German) can be found in `documentation/` in the associated online repository.

### Stable individual characteristics

These predictors are stable individual differences. As a source, we used the ape handbook at Zoo Leipzig. Figure \@ref(fig:demo) gives an overview of the distribution of the different characteristics in the sample. 

#### Group

Group the individual belonged to. Groups were composed of individuals of the same species but because there were two chimpanzee groups (a-chimpanzees and b-chimpanzees), group and species are not equivalent. Variable name in model: `group`.

#### Age

Absolute age of the individual. For some older individuals, only the year of birth was known. In these cases we calculated age with January 1st of that year as the birthday. Variable name in model: `age`.

#### Sex 

Participant's biological sex. Variable name in model: `sex`.

#### Rearing history

Here, we differentiated between, `mother-reared`, `hand-reared` and `unknown`. The last category was used only for three chimpanzees. In the analysis, we classified them as `hand-reared` to facilitate model fitting (i.e. it is very difficult to estimate a parameter for a factor level with so little data). We think this decision is justified because the individuals in question have spent most of their life in close contact to humans and not in a larger chimpanzee group. Variable name in model: `rearing`. 

#### Time lived in Leipzig

Absolute time the individual has lived in Leipzig Zoo. All apes living in Leipzig are involved in behavioral research to a certain degree. Thus, we take this measure to be a rough proxy of how much experience an individual has had with cognitive research. Variable name in model: `time_in_leipzig`.

```{r}
psex <- data_trial%>%
  filter(!is.na(sex))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(group, sex)%>%
  summarise(Frequency = length(unique(subject)))%>%
  ggplot(aes(x = group, y = Frequency, fill = sex))+
  geom_histogram(stat = "identity", position = position_dodge(), col = "black")+
  theme_minimal(base_size = 8)+
  labs(x="", y = "Frequency")+
  scale_fill_grey(name = "Sex")+
  theme(legend.position = c(0.8,0.7), legend.key.size = unit(0.3, "cm"))+
  coord_flip()

prearing <- data_trial%>%
  filter(!is.na(rearing))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(group, rearing,.drop=FALSE)%>%
  summarise(Frequency = length(unique(subject)))%>%
  ggplot(aes(x = group, y = Frequency, fill = rearing))+
  geom_histogram(stat = "identity", position = position_dodge(preserve = "single"), col = "black")+
  theme_minimal(base_size = 8)+
  labs(x="", y = "Frequency")+
  scale_fill_viridis_d(name = "Rearing history")+
  theme(legend.position = c(0.8,0.7), legend.key.size = unit(0.3, "cm"))+
  coord_flip()

page <- data_trial%>%
  filter(!is.na(age))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(group, subject)%>%
  summarise(age = min(age))%>%
  ggplot(aes(x = age, y = group, fill = group))+
  geom_jitter(aes( col = group),alpha = .75, height = 0, width = .05, pch = "|", size = 5)+
  geom_density_ridges(col = "black", alpha = .5)+
  theme_minimal(base_size = 8)+
    labs(y="", x = "Subject age")+
  scale_fill_ptol(name = "Group")+
  scale_color_ptol(name = "Group")+
  guides(fill = F, col = F)

pleipzig <- data_trial%>%
  filter(!is.na(time_in_leipzig))%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(group, subject)%>%
  summarise(time_in_leipzig = min(time_in_leipzig))%>%
  ggplot(aes(x = time_in_leipzig, y = group, fill = group))+
  geom_jitter(aes( col = group),alpha = .75, height = 0, width = .05, pch = "|", size = 5)+
  geom_density_ridges(col = "black", alpha = .5)+
  theme_minimal(base_size = 8)+
    labs(y="", x = "Time lived in Leipzig")+
  scale_fill_ptol(name = "Group")+
  scale_color_ptol(name = "Group")+
  guides(fill = F, col = F)

```

```{r demo, out.width="100%", fig.height = 4.5, fig.cap = "Stable individual characteristics. A) participant sex, B) age distribution by species, C) rearing history, D) time lived in leipzig by species."}

ggarrange(psex, page, prearing, pleipzig, nrow = 2, ncol= 2, widths = c(1,2), labels = c("A","B","C","D"))
```

### Variable individual characteristics

These predictors varied by participant and time point. 

#### Rank

We asked caretakers to order individuals within a given group according to their rank. Ties were allowed. This was done at each time point. An individual's rank was mostly stable (see Figure \@ref(fig:socrel)A) across time points, however, there was some variation. Variable name in model: `rel_rank`.

#### Sickness

As part of the caretakers' daily diary, we asked whether an individual was sick and if yes, how severe the sickness was on a scale from 1 to 7. For each time point, we used the mean of the daily sickness ratings as predictor. Variable name in model: `sick_severity`.

#### Sociality

```{r}
## get date ranges for each time point
# raw_data_date <- data_trial %>%
#   mutate(date = ifelse(group == "b_chimp" & time_point == "13" & date == "20210203", "20210223", date))%>%
#   mutate(date = as.Date(as.character(date), format="%Y%m%d"))%>%
#   select(date, time_point,group)%>%
#   group_by(group,time_point)%>%
#   summarise(end_date = min(date))%>%
#   mutate(time_point = as.numeric(time_point))%>%
#   dplyr::arrange(group,-time_point)%>%
#   mutate(start_date = ifelse(time_point == 1, as.character(as.Date("2020-08-01", format="%Y-%m-%d")), as.character(end_date[-1])))%>%
#   #mutate(dif = time_length(interval(start_date, end_date), "days"))
#   mutate(end_date = as.Date(end_date-1, format = "%Y-%m-%d"),
#          start_date = as.Date(start_date, format = "%Y-%m-%d"),
#          time_point = factor(time_point))%>%
#   group_by(time_point, group)%>%
#   summarise(date = seq(start_date, end_date, by = "days"))%>%
#   dplyr::rename(species = group)%>%
#   mutate(species = paste(str_replace(species,"_","-"),"s",sep =""))
# # 
# # ## get a list of all possible dyady for all possible time points
# dyads <- list.files(path = "../../data/social_network_data",
#                        pattern = "*.csv",
#                        full.names = T)%>%
#   map_df(~read_csv(.))%>%
#   mutate(species = tolower(str_remove(`Configuration Name`, "LAAC ")),
#          session = SessionID,
#          date = as.Date(substr(as.character(DateTime),0,10),format="%Y-%m-%d"),
#          focal = `Focal Name`,
#          associates = `All Occurrence Behavior Social Modifier`)%>%
#   #filter(date2 != "2020-08-21")%>%
#   transform(associates = strsplit(associates,","))%>%
#   unnest(associates)%>%
#   select(species,focal,associates)%>%
#   gather(type, focal, -species)%>%
#   filter(!is.na(focal))%>%
#   distinct(species, focal)%>%
#   mutate(associates = focal)%>%
#   group_by(species)%>%
#   tidyr::expand(focal,associates,c(1:28))%>%
#   mutate(time_point = factor(`c(1:28)`))%>%
#   select(-`c(1:28)`)%>%
#   filter(focal != associates)
# # 
# # ## read in the data from the observational scans
# obs_data <- list.files(path = "../../data/social_network_data",
#                        pattern = "*.csv",
#                        full.names = T)%>%
#   map_df(~read_csv(.))%>%
#   mutate(species = tolower(str_remove(`Configuration Name`, "LAAC ")),
#          session = SessionID,
#          date = as.Date(substr(as.character(DateTime),0,10),format="%Y-%m-%d"),
#          focal = `Focal Name`,
#          associates = `All Occurrence Behavior Social Modifier`)%>%
#   #filter(date2 != "2020-08-21")%>%
#   transform(associates = strsplit(associates,","))%>%
#   unnest(associates)%>%
#   select(species,session,date, focal,associates)%>%
#   left_join(raw_data_date)%>%
#   group_by(species,time_point)%>%
#   mutate(n = length(unique(date)))%>% #compute the number of observations
#   group_by(species,focal, associates,time_point)%>%
#   summarise(count = n(), # count how often a combination of focal and associate occurred for a time point
#             n = max(n))%>%# include the number of observations for that time point
#   mutate(count = ifelse(is.na(associates),0,count))%>%
#   rowwise() %>%
#   filter(!is.na(associates))%>% # remove rows without associates
#   ungroup()
# #   
# # ## merge scan data with the dyads
# raw_srm_data <- dyads %>%
#   left_join(obs_data)%>%
#   group_by(species, time_point)%>%
#   mutate(n = max(n, na.rm = T))%>%
#   filter(n != "-Inf")%>%
#   mutate(count = ifelse(is.na(count),0,count))%>%
#   rowwise() %>%
#   mutate(dyad = paste(sort(c(focal, associates)), collapse = "_"))%>% # create ordered dyad column
#   ungroup()
# # 
# # ## prepare data for model run
# srm_model_data <- raw_srm_data%>%
#   group_by(time_point)%>%
#   distinct(dyad, .keep_all = T)%>% # remove duplicate dyads to avoid counting same dyad twice
#   ungroup()%>%
#   filter(!count > n) # remove rows with more counts than observations
# # 
# # ## run social relations model
# # 
# # ### specify priors
# prior <-  c(prior(normal(0,2), class = Intercept),
#             prior(normal(0,2), class = b),
#             prior(normal(0,1), class = sd)
#             )
# 
# ### run model
# 
# #### phase 1
# srm_1 <-  brm(count | trials(n) ~ species + (0 + time_point | mm(focal, associates)) + ( 0 + time_point | dyad),
#            family = binomial(link = "logit"),
#            data = srm_model_data%>%filter(as.numeric(time_point) < 15),
#            prior = prior,
#            control = list(adapt_delta = 0.95, max_treedepth = 20),
#            iter = 5000, cores = 5, chains = 5)
# 
# #### phase 2
# srm_2 <-  brm(count | trials(n) ~ species + (0 + time_point | mm(focal, associates)) + ( 0 + time_point | dyad),
#            family = binomial(link = "logit"),
#            data = srm_model_data%>%filter(as.numeric(time_point) > 14),
#            prior = prior,
#            control = list(adapt_delta = 0.95, max_treedepth = 20),
#            iter = 5000, cores = 5, chains = 5)
# 
# ### save output
# saveRDS(srm_1, "./saves/srm_1.rds")
# saveRDS(srm_2, "./saves/srm_2.rds")
# 
# ### load model results(too large to put on GitHub)
# srm_1 <-  readRDS("./saves/srm_1.rds")
# srm_2 <-  readRDS("./saves/srm_2.rds")
# 
# 
# ### get species info for each subject
# subject_data <-  raw_srm_data%>%
#   ungroup()%>%
#   select(species, focal, associates)%>%
#   gather(role, subject,-species)%>%
#   distinct(subject, .keep_all = T)%>%
#   select(-role)%>%
#   arrange(species)%>%
#   mutate(subject = tolower(subject))%>%
#   mutate_at(.vars = vars(ends_with("s")),
#             .funs = funs(sub("s", "", .)))%>%
#   mutate(species = str_replace(species, "-", "_"))
# 
# ## extract model estimates for each subject
# ### phase 1
# subject_draws_1 <-  spread_draws(srm_1, r_mmfocalassociates[subject,time_point])%>% # extract posterior draws for each subject and time point
#   dplyr::rename(estimate = r_mmfocalassociates)%>%
#   mutate(subject = tolower(subject))%>%
#   left_join(subject_data) # merge with subject information to get species in
# 
# ### phase 2
# subject_draws_2 <-  spread_draws(srm_2, r_mmfocalassociates[subject,time_point])%>% # extract posterior draws for each subject and time point
#   dplyr::rename(estimate = r_mmfocalassociates)%>%
#   mutate(subject = tolower(subject))%>%
#   left_join(subject_data) # merge with subject information to get species in
# 
# ## summaries subject draws to put on GitHub
# subject_estimates <- bind_rows(
#   subject_draws_1,
#   subject_draws_2)%>%
#   ungroup()%>%
#   mutate(time_point = as.numeric(str_remove(time_point, "time_point")))%>%
#   group_by(species, subject,time_point)%>%
#   summarise(mean = estimate_mode(estimate),
#             uci = hdi_upper(estimate),
#             lci = hdi_lower(estimate))
# 
# ## save subject draws to put on GitHub
# write_csv(subject_estimates, "./saves/srm_estimates.csv")

subject_estimates <- read_csv("./saves/srm_estimates.csv")

## generate plot
plot_est <-  subject_estimates%>%
  ungroup()%>%
  mutate(species_order = as.numeric(as.factor(species)),
         subject = factor(subject),
         subject = reorder(subject, species_order))


psrm <- plot_est%>%
  filter(species == "orangutan" 
         )%>%
ggplot(.,aes(y = mean, x = factor(time_point), col = subject)) +
  geom_point(alpha = .5)+
  geom_vline(xintercept = 14.5)+
  #geom_pointrange(aes(y = mean, ymin = lci, ymax = uci), alpha = .5, position = position_dodge(width = .5))+
  geom_line(aes(group = subject),alpha = .5)+
  facet_grid(~species)+
  theme_minimal()+
  scale_color_viridis_d(name ="Subject")+
  labs(x = "Time point", y = "Sociality estimate")
```

We conducted proximity scans for all groups in the early afternoon on every workday (Monday to Friday). That is, we expect 10 scans for each time point. For each individual, we recorded which individuals were within arms reach. Research assistants used a tablet to record their observations with the behavioral coding software ZooMonitor [@wark2019monitoring]. 

To derive individual specific estimates of sociality for each time point, we fit a variant of a Social Relations Model [@snijders1999social] to the proximity data. These models allow estimating an individual specific sociality index while accounting for the dyadic nature of social interaction. Social relations models usually deal with directed behaviors (e.g. individual *i* is grooming individual *j*). Because the behavior we observed was symmetric, we cannot differentiate between the actor and receiver. @Kajokaite2020.08.04.235788 suggested to speak of a Multiple Membership Relations Model [see also @leckie2019multiple] in such a context, which simply estimates how likely an individual is to be observed in proximity to another individual. 

In `brms` syntax, our model had the following structure: `count | trials(n) ~ group + (time_point | mm(focal, associates)) + (time_point | dyad)`. The dependent variable `count | trials(n)` is the number of times a dyad has been observed (`count`) at a time point relative to the number of scans taken for that time point (`trials(n)`). The fixed effect `group` estimates group difference in sociality. The random effect `(time_point | mm(focal, associates))` estimates the sociality for each individual. In that, the multi-membership grouping term `mm(focal, associates)` captures the fact that the assignment of the two roles (focal and associate) is arbitrary in the context of a symmetric behavior. The random slope `time_point` (treated as a factor) allowed us to estimate sociality for each time point. Finally, the random effect `(time_point | dyad)` accounts for dyad composition; in some cases a particular dyad composition (e.g. mother and infant) might be sufficient to explain high levels of sociality in an individual.

For each individual and time point, we extracted the sociality estimates and used them to predict cognitive performance in the different tasks for that time point. Figure \@ref(fig:socrel)B visualizes the sociality measures for one group across the different time points. Variable name in model: `sociality`.     

```{r, out.width="100%", fig.height = 4, fig.cap = "Stable individual characteristics. A) participant sex, B) age distribution by species, C) rearing history, D) time lived in leipzig by species."}
prank <- data_trial%>%
     mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species))%>%
  group_by(subject, group)%>%
  summarise(sd = sd(rank, na.rm = T))%>%
  ggplot(aes(x = sd, y = group, fill = group))+
  geom_jitter(aes(col = group), height = 0, width = .05, pch = "|", size = 5)+
  geom_density_ridges(alpha = .5, col = "black")+
  labs(x ="Variability in rank (sd)", y = "")+
  theme_minimal()+
  scale_fill_ptol()+
  scale_colour_ptol()+
  scale_x_continuous(limits = c(0, 3), oob = scales::squish)+
  guides(col = F, pch = F, fill =F)
```

```{r socrel, out.width="100%", fig.height = 2.5, fig.cap = "Variable individual characteristics. A) variability in rank (caretaker ratings) for each subject and species, B) sociality estimates for orangutans based on Multiple Membership Relations Model."}
ggarrange(prank,psrm, labels = c("A","B"), widths = c(1,1.5))
```

### Group life

These predictors varied by time point and group, but were the same for all individuals in that group. They were recorded in the animal caretaker diary. Figure \@ref(fig:glife) visualizes the different variables across time points.

#### Time outdoors

Each day, the animal caretakers noted in the diary how many hours each group spent in the outdoor enclosure instead of the indoor enclosure or the sleeping rooms. To compute the predictor, we averaged across these values for each time point and group. Variable name in model: `time_outdoors`.

#### Disturbances

The animal caretakers also noted if there were any unusual disturbances for a particular group. Examples were construction works in the building, heavy weather conditions or green-keeping activities. In addition, the caretakers rated how disturbing they judged these events to be on a scale from 1 to 7. For each time point, we calculated the mean of these ratings. Variable name in model: `dist_mean`.

#### Life events

This variable captured whether there were any notable events within the group. Examples were fights in the group or the temporal removal of some individuals for medical procedures. Again, we asked the caretakers to rate the severity of these events on a scale from 1 to 7 and averaged across them. Variable name in model: `le_mean`.  


```{r}
plife <- data_trial%>%
  group_by(time_point,group)%>%
  summarise(time_outdoors = mean(time_outdoors),
            disturbance = mean(dist_mean),
            life_event = mean(le_mean))%>%
  pivot_longer(names_to = "measure", values_to = "value", cols = c(-group, -time_point))%>%
  ggplot(aes(x = time_point, y = value, col = group))+
  geom_vline(xintercept = 14.5)+
  geom_point()+
  geom_line(aes(group = group), alpha = .75)+
  facet_grid(measure~., scales = "free_y")+
  labs(x = "Time Point", y = "")+
  scale_color_ptol(name = "Group")+
  theme_minimal()

```

```{r glife, out.width="100%", fig.height = 3, fig.cap = "Variation in group life related measures across groups and time points."}
plife
```

### Testing arrangements

Testing arrangements varied between individuals, sessions and time points. The experimenter recorded them either based on their observations during testing or from the testing schedule, which lists all studies along with their participants that take place on a particular day.  

#### Observer

We noted whether or not there was another animal in the same room or the room adjacent to the one the participant was in. Variable name in model: `observer`. 

#### Study on same day

This predictor recorded whether or not the participant had already participated in a different study on the same day. The experimenter took this information from the testing schedule. Variable name in model: `test_day`. 

#### Studies since last time point

Here we counted in how many other studies the participant had taken part in since the last time they were tested in that particular task. The experimenter took this information from the testing schedule. Variable name in model: `test_tp`.  

# Analytical framework

We had two overarching questions. On the one hand, we were interested in the cognitive measures and the relations between them. That is, we asked how stable performance in a given task was on a group-level, how stable individual differences were, how reliable the measures were. We also investigated relations between the different tasks. We used *Structural Equation Modeling* (SEM) [@bollen1989structural; @hoyle2012handbook] to address these questions. SEMs usually require larger sample sizes than available in the present study. In the [appendix](#appendix) we present results from a small simulation study which show that parameters in the employed SEMs are accurately estimated using Bayesian estimation techniques given our available sample sizes under reasonable model restrictions. We lay out the restrictive assumptions we imposed on the parameters in the text below.

Our second question was, which predictors explain variability in cognitive performance. Here we wanted to see which of the predictors we recorded were most important to predict performance over time. This is a variable selection problem (selecting a subset of variables from a larger pool) and we used *Projection Prediction Inference* for this [@piironen2018projective]. 

## Structural equation modeling

In the present analyses we were interested in estimating the stability of performances in a given task across time as well as the association between performances across different tasks (on a stable as well as the time-point specific level). To separate components of random fluctuation (measurement error) from systematic differences in performance across time, we used Structural equation models (SEM). SEMs can be used to model relations between latent variables (constructs) which are estimated based on several observed variables. 

We used SEM to estimate traits (stable over time) and states (time varying). In the present context, one can think of a trait as a stable psychological ability (e.g. ability to make causal inferences) and states as time-specific, variable psychological conditions (e.g. variations in performance due to being attentive or inattentive). Variation in performance on a given time point can then be partitioned into variance explained by the trait, variance explained by the situation or individual-situation interactions, and measurement error. Because the latent variables are estimated on multiple indicators, they are assumed to be measurement-error free [@steyer1992states; @steyer2015theory; @geiser2020longitudinal]. Next we describe the model construction process in more detail.

At each time point, we observed several identical trials per individual per task. Using the individual trials (8 to 12 dichotomous items, depending on the task) as indicators for a latent ability factor per time point (i.e., assuming a Rasch model per time point) in a longitudinal SEM resulted in estimation problems due to many empty cells in the bivariate distributions across time and / or tasks. Therefore we decided to model sum scores of the repeated trials, given that each trial per task was an identical repetition of the same task.

To separate reliable from unreliable variance components and obtain reliability estimates of the resulting sum score variables, we build two sum score variables per task per time point. That is, for each task, two parallel test halves were build, corresponding to performance sum scores of half of the trials of the same time point per task. Trials were alternately assigned to the first and the second test half. For tasks with 12 trials per time point this procedure resulted in two test halves assuming 7 possible values (0 to 6 correctly solved trials), for tasks with 8 trials per time point, test halves could maximally assume 5 possible values (0 to 4 correctly solved trials). 

The two test halves served as indicators for a common latent construct per time point, assuming parallel test halves (i.e., factor loadings set to 1 and assuming equal reliability). Due to only few different observed values and skewed distributions of the sum score variables, indicators were modeled as ordered categorical variables, using a probit link function. The models thereby correspond to normal-ogive graded response models [@samejima1969estimation; @samejima1996graded]. That is, the models assume a continuous latent ability underlying the discrete responses, with an increasing probability of more correctly solved trials with increasing ability.

For model parsimony, to improve estimation accuracy (see simulation studies) and in order to test for latent mean differences across time, we assume strict factorial (or measurement) invariance across time [@meredith1993measurement; @millsap2004assessing]. 
That is, in each model (task), loading parameters are set to 1 at all time points, residual variances are equal to 1 (by definition of the graded response model as detailed below), and threshold parameters (see below for details) are set invariant across time points. In other words, we assume that the indicators (test halves) measure the latent variable in an equivalent and stable manner over time.


### Models and coefficients

For each task, we constructed three different models which increased in complexity. We started with a latent state (LS) model, which estimates a latent state for each time point based on the two test halves. Stability of group level performance can be assessed by comparing latent state means across time points. Stability of individual differences can be assessed by correlating latent state variables across different time points.

Second, we fit a latent state-trait (LST) model. In LST models, true inter-individual differences are decomposed into a latent trait variable and time-specific deviations of the true score from the latent trait (state residual variable). In the following LST models, we assume stable latent trait variables across time (no trait change). The model allows us to partition the true variance in performance into stable (trait) and variable (state residual) components. Assuming a stable latent trait variable, the LST model is more restrictive than the LS model with respect to the implied covariance matrix, as correlations between true scores are not freely estimated across time points but assumed to be the same for different time lags.   

Finally, we fit an LST model with autoregressive effects (LST-AR). In addition to the LST model architecture, this model assumes that the state residual variables at one time point can be used to predict the true score at the subsequent time point. As such, it captures the idea that measurements that are closer in time are likely to be more highly correlated and quantifies potential carry-over effects from one time point to the next. The following sections give a mathematical description of the different models and the parameters in them.

#### Latent State Models

In the following we chose a factor analytical representation of the graded response model, that is, we present the models as factor models for ordinal data. Thereby we assume that the observed categorical variables $Y_{it}$ for test half $i$ at time point $t$ result from a categorization of unobserved continuous latent variables $Y^*_{it}$ which underlie the observed categorical variables. For observed variables that take on $k_{it}$ different ordered values out of the set of possible categories $S_{it} = 0, ..., k_{it}-1$ the relation between $Y_{it}$ and $Y^*_{it}$ is described by:

\begin{equation}
Y_{it}=
\begin{cases} 
0~ & for ~~~Y_{it}^*\le \kappa_{1it} \\
s~~~~~~~~~ & for ~~~\kappa_{sit}<Y_{it}^*\le \kappa_{(s+1)it}~~~~~with~~0 < s < k_{it}-1\\
k_{it} - 1~~~ & for ~~~ \kappa_{(k_{it}-1)it}<Y_{it}^*
\end{cases}
(\#eq:eq0-1)
\end{equation}

where $\kappa_{sit}$ denote threshold parameters [@muthen1984general].

The graded response model assumes that the different categories of responses (in our case the number of correct trials per test half) form an ordered scale. Which category an individual scores depends on their latent ability. Because the latent variable is continuous but the response is discrete, there are thresholds on the latent ability that mark the transition between response categories. The threshold parameters $\kappa_{sit}$ correspond to the level of the latent ability necessary to respond in category $s$ or higher with 0.50 probability. 

In latent state models, these continuous latent variables $Y^*_{it}$ are decomposed into a latent state variable $S_t$ and a measurement error variable $\epsilon_{it}$ [see, for instance @eid2014statistical]:

\begin{equation}
Y^*_{it}= S_t + \epsilon_{it}
(\#eq:eq1)
\end{equation}

with $\epsilon_{it} \sim N(0,1)$ $\forall~ i,t$ (probit parameterization; normal-ogive graded response model). See @takane1987relationship for the equivalence of the normal-ogive graded response model and the factor model with ordinal indicators.

At each time point $t$, the two latent variables $Y^*_{1t}$ and $Y^*_{2t}$ are assumed to capture a common latent state variable $S_t$. Latent state variables are allowed to freely correlate across time, with latent (measurement-error free) correlations serving as indirect indicators of stability across time. The model is depicted for six measurement time points in Figure \@ref(fig:lsgraph). 

To test for possible mean changes of ability across time, the means of the latent state variables are freely estimated (assuming invariance of the threshold parameters $\kappa_{sit}$ across time).

As an estimate of reliability, the proportion of true score variance relative to the total variance of the continuous latent variables $Y^*_{it}$ is computed:

\begin{equation}
Rel(Y^*_{it})=\frac{Var(S_t)}{Var(S_t)+Var(\epsilon_{it})}=\frac{Var(S_t)}{Var(S_t)+1}
(\#eq:eq1-1)
\end{equation}

```{r lsgraph, engine='tikz', fig.cap="Latent State model for two indicators and six measurement time points.", out.width="25%", fig.align='center'}

\usetikzlibrary{arrows} 
\usetikzlibrary{positioning} 
\usetikzlibrary{shapes} 
\usetikzlibrary{fit} 
\usetikzlibrary{backgrounds} 
\usetikzlibrary{calc}

\begin{tikzpicture}
        [manifest/.style={rectangle,draw=black!80,semithick,minimum width=1.5cm,inner sep=
4pt,text centered},
        latent/.style={ellipse,draw=black!80,thick,inner sep=2,text centered},
        on/.style={->,>=stealth',semithick},
        from/.style={<-,>=stealth',semithick},
        with1/.style={<->,>=stealth',semithick,bend left=30},
        with2/.style={<->,>=stealth',semithick,bend right=30},
        with3/.style={<->,>=stealth',semithick,bend left=50},]

% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o1) at (0,0) {$S_{1}$};
                \node[latent] (o2) [below=of o1] {$S_{2}$};
                \node[latent] (o3) [below=of o2] {$S_{3}$};
                \node[latent] (o4) [below=of o3] {$S_{4}$};
                \node[latent] (o5) [below=of o4] {$S_{5}$};
                \node[latent] (o6) [below=of o5] {$S_{6}$};
\end{scope}
                                                             	                        
% Manifest % 
 \foreach \a in {1,...,6} {\node (y1\a) [right = 2 of o\a] (yhelp\a) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b] (y1\b) {$Y^*_{1\b}$}
 	edge [from] node[above] {1} (o\b);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c] (y2\c) {$Y^*_{2\c}$}
 	edge [from] node[below] {$1$} (o\c);} 

% Fehler %
\foreach \a in {1,...,5} {\node (e1\a) [below right = .3 of y1\a] {}
	edge [on] (y1\a.south east);}
\foreach \b in {1,...,5} {\node (e2\b) [below right = .3 of y2\b] {}
	edge [on] (y2\b.south east);}

\node (e16) [below right = .3 of y16] {$\epsilon_{16}$}
	edge [on] (y16.south east);
\node (e26) [below right = .3 of y26] {$\epsilon_{26}$}
	edge [on] (y26.south east);

%correlations%
\foreach \a in {1,...,5} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o6);}
\foreach \a in {1,...,4} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o5);}
\foreach \a in {1,...,3} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o4);}
\foreach \a in {1,...,2} {
\draw[<->,>=stealth',semithick] (o\a) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o3);}      
\draw[<->,>=stealth',semithick] (o1) to [with2]
node[align=center,below,yshift= 0mm, pos=0.5] 
                      {}  (o2);                

\end{tikzpicture}

```

#### Latent State-Trait (LST) models


In LST models, the latent state variables $S_{it}$ are further decomposed into a latent trait variable $T_{it}$ and a latent state residual variable $\zeta_{it}$. The latent trait variables $T_{it}$ are time-specific dispositions, that is, trait scores capture the expected value of the latent state (i.e., true score) variable for an individual at time $t$ across all possible situations the individual might experience at time $t$ [@eid2017definition; @steyer2015theory]. The state residual variables $\zeta_{it}$ capture the deviation of a momentary state from the time-specific disposition $T_{it}$. In the following models, we assume that latent traits are stable across time. Additionally assuming common latent trait and state residual variables across the two test halves, results in the following measurement equation for parcel $i$ at time point $t$:

\begin{equation}
Y^*_{it}= T + \zeta_t + \epsilon_{it}
(\#eq:eq2)
\end{equation}

Here, $T$ is a stable (time-invariant) latent trait variable, capturing stable interindividual differences between individuals. The state residual variable $\zeta_t$ captures time-specific deviations of the respective true score from the trait variable at time $t$, and thereby captures deviations from the trait due to situation or person-situation interaction effects. $\epsilon_{it}$ denotes a measurement error variable, with $\epsilon_{it} \sim N(0,1)$ $\forall~ i,t$. The model is depicted for 6 measurement time points in Figure \@ref(fig:lstgraph).  

```{r lstgraph, engine='tikz', fig.cap="Latent State-Trait model for two indicators and six measurement time points. All factor loadings of the latent trait factor $T$ are fixed to 1 (not displayed in the figure)", out.width="30%", fig.align='center'}

\usetikzlibrary{arrows} 
\usetikzlibrary{positioning} 
\usetikzlibrary{shapes} 
\usetikzlibrary{fit} 
\usetikzlibrary{backgrounds} 
\usetikzlibrary{calc}

\begin{tikzpicture}
        [manifest/.style={rectangle,draw=black!80,semithick,minimum width=1.5cm,inner sep=
4pt,text centered},
        latent/.style={ellipse,draw=black!80,thick,inner sep=2,text centered},
        on/.style={->,>=stealth',semithick},
        from/.style={<-,>=stealth',semithick},
        with1/.style={<->,>=stealth',semithick,bend left=30},
        with2/.style={<->,>=stealth',semithick,bend right=30},
        with3/.style={<->,>=stealth',semithick,bend left=50},]
        
% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o1) at (0,0) {$\zeta_{1}$};
                \node[latent] (o2) [below=of o1] {$\zeta_{2}$};
                \node[latent] (o3) [below=of o2] {$\zeta_{3}$};
                \node[latent] (o4) [below=of o3] {$\zeta_{4}$};
                \node[latent] (o5) [below=of o4] {$\zeta_{5}$};
                \node[latent] (o6) [below=of o5] {$\zeta_{6}$};
\end{scope}
                     
% Trait % 
\node[latent,node distance=7] (t) [right=of  o4]  {$T_{}$};
                                             	                        
% Manifest % 
 \foreach \a in {1,...,6} {\node (y1\a) [right = 2 of o\a] (yhelp\a) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b] (y1\b) {$Y^*_{1\b}$}
 	edge [from] node[above] {1} (o\b);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c] (y2\c) {$Y^*_{2\c}$}
 	edge [from] node[below] {$1$} (o\c);} 

% Fehler %
\foreach \a in {1,...,5} {\node (e1\a) [below right = .3 of y1\a] {}
	edge [on] (y1\a.south east);}
\foreach \b in {1,...,5} {\node (e2\b) [below right = .3 of y2\b] {}
	edge [on] (y2\b.south east);}

\node (e16) [below right = .3 of y16] {$\epsilon_{16}$}
	edge [on] (y16.south east);
\node (e26) [below right = .3 of y26] {$\epsilon_{26}$}
	edge [on] (y26.south east);

% Pfeile %
\foreach \a in {1,...,6}
	\path (y1\a.0) edge [from] (t);
\foreach \a in {1,...,6}	
	\path (y2\a.0) edge [from] (t);
\end{tikzpicture}

```

As noted above, we assume strict factorial (measurement) invariance. Additionally, for reasons of parsimony, we assume that the variances of the state residual variances are invariant across time. As a consequence, the specified LST model corresponds to a multilevel model with a latent trait factor at the between-level (person-level) and a latent state residual factor at the within-level (time-specific) level.

The following variance components can be computed for the presented LST model.

##### Consistency

Proportion of true variance (i.e., measurement-error free variance) that is due to true inter-individual stable trait differences.

\begin{equation}
Con(Y^*_{it})=\frac{Var(T)}{Var(T)+Var(\zeta_t)}
(\#eq:eq3)
\end{equation}

##### Occasion specificity

Proportion of true variance (i.e., measurement-error free variance) that is due to true inter-individual differences in the state residual variables (i.e., occasion-specific variation not explained by the trait).

\begin{equation}
OS(Y^*_{it})=1-Con(Y^*_{it}) = \frac{Var(\zeta_t)}{Var(T)+Var(\zeta_t)}
(\#eq:eq4)
\end{equation}

As state residual variances $Var(\zeta_t)$ were set equal across time, $OS(Y^*_{it})$ is constant across time (as well as across item parcels $i$). 

#### Latent State-Trait models with autoregressive effects (LST-AR)

The following model is a restrictive variant of the model described in @eid2017definition. The model is depicted for six measurement time points in Figure \@ref(fig:lstrgraph).

```{r lstrgraph, engine='tikz', fig.cap="Latent State-Trait model with autoregressive effects for two indicators and six measurement time points. All factor loadings of the latent trait factor $T$ are fixed to 1 (not displayed in the figure)", out.width="35%", fig.align='center'}

\usetikzlibrary{arrows} 
\usetikzlibrary{positioning} 
\usetikzlibrary{shapes} 
\usetikzlibrary{fit} 
\usetikzlibrary{backgrounds} 
\usetikzlibrary{calc}

\begin{tikzpicture}
 [manifest/.style={rectangle,draw=black!80,semithick,minimum width=1.5cm,inner sep=
4pt,text centered},
        latent/.style={ellipse,draw=black!80,thick,inner sep=2,text centered},
        on/.style={->,>=stealth',semithick},
        from/.style={<-,>=stealth',semithick},
        with1/.style={<->,>=stealth',semithick,bend left=30},
        with2/.style={<->,>=stealth',semithick,bend right=30},
        with3/.style={<->,>=stealth',semithick,bend left=50},]


% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o1) at (0,0) {$\zeta_{1}$};
                \node[latent] (o2) [below=of o1] {$O_{2}$}
                edge [from] node[right] {$\beta$} (o1);
                \node[latent] (o3) [below=of o2] {$O_{3}$}
                edge [from] node[right] {$\beta$} (o2);
                \node[latent] (o4) [below=of o3] {$O_{4}$}
                edge [from] node[right] {$\beta$} (o3);
                \node[latent] (o5) [below=of o4] {$O_{5}$}
                edge [from] node[right] {$\beta$} (o4);
                \node[latent] (o6) [below=of o5] {$O_{6}$}
                edge [from] node[right] {$\beta$} (o5);
\end{scope}

\foreach \a in {2,...,6} {\node [below left = .4 of o\a] (zeta\a) {$\zeta_{\a}$}
	edge [on] (o\a);}
                     
% Trait % 
\node[latent,node distance=7] (t) [right=of  o4]  {$T_{}$};
                                             	                        
% Manifest % 
 \foreach \a in {1,...,6} {\node (y1\a) [right = 2 of o\a] (yhelp\a) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b] (y1\b) {$Y^*_{1\b}$}
 	edge [from] node[above] {1} (o\b);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c] (y2\c) {$Y^*_{2\c}$}
 	edge [from] node[below] {$1$} (o\c);} 

% Fehler %
\foreach \a in {1,...,5} {\node (e1\a) [below right = .3 of y1\a] {}
	edge [on] (y1\a.south east);}
\foreach \b in {1,...,5} {\node (e2\b) [below right = .3 of y2\b] {}
	edge [on] (y2\b.south east);}

\node (e16) [below right = .3 of y16] {$\epsilon_{16}$}
	edge [on] (y16.south east);
\node (e26) [below right = .3 of y26] {$\epsilon_{26}$}
	edge [on] (y26.south east);

% Pfeile %
\foreach \a in {1,...,6}
	\path (y1\a.0) edge [from] (t);
\foreach \a in {1,...,6}	
	\path (y2\a.0) edge [from] (t);
\end{tikzpicture}

```

Measurement equation for parcel $i$ at time point $t$:

\begin{equation}
Y^*_{it}= T + O_t + \epsilon_{it}
(\#eq:eq5)
\end{equation}

where $T$ is the latent trait variable, the occasion-specific variables $O_t$ capture time-specific deviations of the respective true score from the latent trait variable $T$, and $\epsilon_{it}$ is a measurement error variable,  with $\epsilon_{it} \sim N(0,1) ~\forall i,t$. $O_t$ is assumed to follow an autoregressive process of order 1 across time (within subjects), that is:

\begin{align}
O_t  &= \zeta_t ~~~~~~~~t = 1 \notag \\
O_t  &= \beta O_{(t-1)} + \zeta_t~~~~~~~~t > 1 \notag
\end{align}

where the latent state residual variables $\zeta_t$ capture true time-specific inter-individual differences that cannot be explained based on the true score of previous measurement time points. We make the same assumptions about factorial invariance as in the LST model.  

The following variance coefficients can be computed. Note that the names of the coefficients used here slightly differ from those used in @eid2017definition.

##### Consistency

Proportion of true variance (i.e., measurement-error free variance) that is due to true inter-individual stable trait differences.

\begin{equation}
Con(Y^*_{it})=\frac{Var(T)}{Var(T)+\beta^2 Var(O_{(t-1)})+Var(\zeta_t)}
(\#eq:eq6)
\end{equation}

##### Occasion specificity

Proportion of true variance (i.e., measurement-error free variance) that is due to true inter-individual differences in the state residual variables, that is occasion-specific variation that is not explained by the autoregressive process or the trait variable $T$.

\begin{equation}
OS(Y^*_{it}) = \frac{Var(\zeta_t)}{Var(T)+\beta^2 Var(O_{(t-1)})+Var(\zeta_t)}
(\#eq:eq7)
\end{equation}

As the proportion of variance explained by the autoregressive process stabilizes over time, all coefficients have converged to a relatively stable value at $t=14$, indicating the long-term proportions of variance that are to be expected.

##### Autoregressive predictability

Proportion of true variance that is explained by carry-over effects from previous measurement time points:

\begin{equation}
Pred(Y^*_{it}) = \frac{\beta^2 Var(O_{(t-1)})}{Var(T)+\beta^2 Var(O_{(t-1)})+Var(\zeta_t)}
(\#eq:eq8)
\end{equation}

#### Models for combinations of tasks

To investigate associations between cognitive performance in different tasks, the described models were extended to multitrait models. Due to the small sample size, we could not combine all tasks in a single, structured model. Instead, we assessed relations between tasks in pairs. 

### Estimation

Models were estimated with MPlus version 8.4 [@muthen1998mplus], using Bayesian Markov-Chain Monte-Carlo sampling, with the Mplus default priors (see simulation studies in the [appendix](#appendix)). Using inverse gamma priors `IG(0.001, 0.001)` for LST models did not substantially change the parameter estimates (see simulation study). Therefore, only the results using the MPlus default priors are reported. We used two chains with a minimum of 10,000 iterations per chain, with a thinning of 10 (corresponds to a minimum of 100,000 drawn samples per chain of which every 10th is used for the construction of the posterior distribution). The first half of each chain is discarded as burn-in. Convergence was assumed and estimation stopped when the Potential Scale Reduction (PSR) factor was well below a threshold of 1.01 for the first time after the minimum number of iterations was reached.

Model fit was evaluated by computing Posterior Predicted P-values (PPP). PPP is the probability that the newly generated data are more extreme than the observed data, as measured by a specific test statistic or discrepancy function, in this case the chi-square fit function (that is, the likelihood ratio test between the specified structural equation model and an unrestricted mean and variance covariance model), see @asparouhov2010bayesian. The PPP is computed via the following steps: For a given MCMC iteration, a new data set is generated based on the model and the parameters of that iteration. Then the likelihood ratio chi-square test is applied to the real data as well as the newly generated data set to compute a fit index. The indices for the data and the generated data are then compared in size. If the value for the data is smaller, it is scored as 1 and if not, as 0. Averaging across these scores for the different iterations yields the PPP. Thus, values around .5 suggest a good model fit (no systematic difference between real and generated data) and very high and very low values suggest a poor model fit and / or model misspecification. In addition, we report the 95% CI of the difference between predicted and observed chi-square values, which should be centered around 0 for a good model fit.

In Mplus, every 10th iteration after burn-in is used to compute the PPP and the underlying continuous response variables $Y^*$ are used to compute the PPP in case of ordinal data.

## Projection predictive inference

Our goal is to select the predictor variables that are relevant for predicting performance in the different cognitive tasks over time. The selection of relevant predictor variables constitutes a variable selection problem, for which a range of different methods are available (e.g., shrinkage priors). We chose to use projection predictive inference because it is a state-of-the-art variable selection procedure that provides an excellent trade-off between model complexity and accuracy [@piironen2017comparison], especially when the goal is to identify a minimal subset of predictors that yield a good predictive model [@pavone2020using]. 

An overview of different projection techniques and an introduction to the projection prediction approach for generalized linear models can be found in @piironen2018projective. In this work, we use the extension to the generalized linear multilevel model case provided by @catalina2020projection.    
The projection prediction approach can be viewed as a two-step process: The first step consists of building the best predictive model possible, called the reference model. In the context of this work, the reference model is a Bayesian multilevel regression model (repeated measurements nested in apes), including all available predictors.   
In the second step, the goal is to replace the posterior distribution of the reference model with a simpler distribution. This is achieved via a forward step-wise addition of predictors that decrease the Kullback-Leibler (KL) divergence from the reference model to the projected model. Let the reference model and the projected model have parameters \(\theta'\) and \(\theta\) respectively. Then by the definition of the KL divergence, the following optimization problem is obtained
\begin{equation}
\begin{aligned}
{\theta}_{\perp} &=\arg \min _{\theta} \frac{1}{n} \sum_{i=1}^{n} \mathrm{KL}\left(p\left(\tilde{y}_{i} \mid \theta'\right) \| p\left(\tilde{y}_{i} \mid \theta\right)\right) \\
&=\arg \min _{\theta} \frac{1}{n} \sum_{i=1}^{n} p\left(\tilde{y}_{i} \mid \theta'\right) \cdot \log \left(\frac{p\left(\tilde{y}_{i} \mid \theta'\right)}{p\left(\tilde{y}_{i} \mid {\theta}\right)}\right)
(\#eq:eq9)
\end{aligned}
The result is a list containing the best model for each number of predictors from which the final model is selected by inspecting the mean log-predictive density (`elpd`) and root-mean-squared error (`rmse`). The projected model with the smallest number of predictors is chosen, which shows similar predictive performance as the reference model.

We built five different (Bayesian multilevel regression) reference models and ran them through the above-described projection prediction approach in phase 1 and phase 2 for the five tasks: Gaze following, causal inference, inference by exclusion, quantity discrimination, and delay of gratification. The dependent variable for each task was the cognitive performance of the apes, that is, the number of correctly solved trials per time point and task. The model for the delay of gratification task was only estimated once (phase 2).
Continuous predictors were centered when needed. We transformed the apes rank variable into a relative rank, where a rank of value one depicts a subject with the highest possible rank. We added the predictor `day2` for gaze following, indicating whether the trials were from the second session or the first. All reference models converged well, having no divergent transitions, R-hat values equal to 1, and large ESSs for virtually all parameters. The R-hat value is a diagnostic value to investigate the convergence of the model and refers to the same concept as the potential scale reduction (PSR) factor defined above. R-hat values close to 1 indicate that the chains have mixed well (the estimates of the chains agree with each other), while values above 1 indicate that the chains did not converge to the same value. For chains of autocorrelated samples, the effective sample size (ESS) is an estimate for the number of independent samples within a chain containing the same amount of information about the dependent variable.

Following step two, we performed projection prediction for each reference model separately, thus resulting in five different rankings for the relevant predictors. We used the R package `projpred` [@projpred202], which implements the aforementioned projection prediction technique. The predictor relevance ranking is measured by the LOO cross-validated mean log-predictive density and root-mean-squared error. To find the optimal submodel size, we inspected -- in line with the authors' recommendations -- summaries and the plotted trajectories of the calculated `elpd` and `rmse`.

The order of relevance for the predictors and the random intercept (together called terms) is created by performing forward search. The term that decreases the KL divergence between the reference model's predictions and the projection's predictions the most goes into the ranking first. Forward search is then repeated $N$ times to get a more robust selection. We chose the final model by inspecting the predictive utility of each projection. To be precise, we chose the model with $p$ terms where $p$ depicts the number of terms at the cutoff between the term that increases the `elpd` and the term that does not increase the `elpd` by any significant amount. In order to get a useful predictor ranking, we manually delayed the random intercept term to the last position in the predictor selection process. The random intercept delay is needed because if the random intercept were not delayed, it would soak up almost all of the variance of the dependent variable before the predictors are allowed to explain some amount of the variance themselves. One could have used the function `suggest_size` as a heuristic decision rule to find the optimal submodel as an alternative to a graphical inspection. However, this is not yet possible due to the delay of the random intercept term. 

# Results

```{r}
time_plot_1 <- data_task%>%
 filter(task != "switching_feature",
         task != "switching_place")%>%
  mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species),
         group = factor(group))%>%
  drop_na(performance)%>%
  #mutate(time_point = factor(time_point))%>%
  group_by(group, task,time_point, species)%>%
  summarise(mean = mean(performance,na.rm = TRUE))

switch_data <- data_trial%>%
  filter(grepl("switch", task))%>%
  group_by(task, time_point)%>%
  tidyboot_mean(col = code, na.rm = TRUE)%>%
  mutate(phase = str_remove(task,"switching_"),
         task = "switching")

time_plot_2 <- data_task%>%
    filter(task != "switching_feature",
         task != "switching_place")%>%
  mutate(group = as.character(group), 
         species = ifelse(grepl("chimp",group),"chimpanzee", group), 
         species = factor(species),
         group = factor(group))%>%
  drop_na(performance)%>%
  #mutate(time_point = factor(time_point))%>%
  group_by(task, time_point)%>%
  tidyboot_mean(col = performance, na.rm = TRUE)%>%
  mutate(n = n(),
         chance = ifelse(task == "switching", 1/3, ifelse(task == "gaze_following" | task == "delay_of_gratification", NA,0.5)))
```

```{r}
perfplot <- ggplot()+
  facet_wrap_custom(task~., scales = "free_y", ncol = 2, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0, 1))),
    scale_override(2, scale_y_continuous(limits = c(0, 1))),
    scale_override(3, scale_y_continuous(limits = c(0, 1))),
    scale_override(4, scale_y_continuous(limits = c(0, 1))),
    scale_override(5, scale_y_continuous(limits = c(0, 1))),
    scale_override(6, scale_y_continuous(limits = c(-1, 1)))
  ))+
  geom_vline(xintercept = 14.5)+
  geom_hline(data = time_plot_2,aes(yintercept = chance), lty = 2, col = "black", alpha = .75)+
  geom_point(data = time_plot_1, aes(x = time_point, y = mean,  col = group), alpha = .6, size = 0.75)+
  geom_line(data = time_plot_1, aes(x = time_point, y = mean, col = group, group = group), alpha = .6, size = 0.5)+
  geom_pointrange(data = time_plot_2, aes(x = time_point, y = mean, ymin = ci_lower, ymax = ci_upper),pch = 4, size = 0.5, alpha = .6)+
  geom_line(data = time_plot_2, aes(x = time_point, y = mean, group = task), size = 0.5, alpha = .5)+
  theme_minimal(base_size = 8)+
  labs(x = "Time Point", y = "Performance")+
  guides(size = F)+
  scale_shape(name = "Switching Phase")+
  scale_color_ptol(name = "Group")+
  theme(legend.position = "bottom", legend.direction = "horizontal")
```

```{r perfplot, fig.height=6, fig.cap = "Results from the five cognitive tasks across time points. Black crosses show mean performance at each time point across species (with 95\\% CI). Colored dots show mean performance by species. Dashed line shows the chance level whenever applicable. The vertical back line marks the transition between phase 1 and 2.", out.width="100%"}
# ggarrange(perfcaus, perfinf, perfquant, perfgaze, perfswitch, common.legend = T, ncol= 1,  labels = c("A","B","C","D","E"), legend = "right")

perfplot
```

```{r}
cor_data <- bind_rows(
    data_task %>%
    filter(time_point < 15)%>%
  arrange(task)%>%
  drop_na(performance)%>%
  select(subject, task, time_point, performance)%>%
  pivot_wider(values_from = "performance", names_from = "time_point")%>%
  ungroup()%>%
  select(-subject)%>%
  group_by(task)%>%
  group_split(.keep = F)%>%
  setNames(data_task%>%filter(time_point < 15)%>%arrange(task)%>%distinct(task)%>%pull(task))%>%
  purrr::map(cor_func)%>%
  melt()%>%
  mutate(task = factor(L1))%>%
  filter(task != "switching_place",
         task != "switching_feature")%>%
  mutate(phase = "Phase 1"), 
  
data_task %>%
  filter(time_point > 14)%>%
  arrange(task)%>%
  drop_na(performance)%>%
  select(subject, task, time_point, performance)%>%
  pivot_wider(values_from = "performance", names_from = "time_point")%>%
  ungroup()%>%
  select(-subject)%>%
  group_by(task)%>%
  group_split(.keep = F)%>%
  setNames(data_task%>%filter(time_point > 14)%>%arrange(task)%>%distinct(task)%>%pull(task))%>%
  purrr::map(cor_func)%>%
  melt()%>%
  mutate(task = factor(L1))%>%
  mutate(phase = "Phase 2")
  
  )

cor_data_sum <- cor_data%>%
  filter(task != "switching_place",
         task != "switching_feature")%>%
  group_by(phase, task)%>%
  mean_hdci(value)
  
# ggplot(cor_data, aes(x = value, col = task, fill = task))+
#   #geom_histogram(col = "black", fill = "white")+
#   geom_density(alpha = .3)+
#   #xlim(-0.5,1)+
#   #facet_wrap(~task)+
#   labs(x = "Correlation Coefficient", y = "")+
#   theme_minimal()+
#   scale_color_colorblind(name = "Task")+
#   scale_fill_colorblind(name = "Task")


rel_plot <- ggplot(data = cor_data, aes(x = value , y = reorder(task, desc(task)) ,fill = task)) +
  geom_vline(xintercept = 0, color = "black", size = .5, lty = 2) +
  geom_density_ridges(rel_min_height = 0.01,col = "black", scale = 2,
                      alpha = 0.5) +
  geom_pointinterval(data = cor_data_sum, aes(xmin = .lower, xmax = .upper), size = 1)+
  labs(x = "Correlation Coefficient",
       y = element_blank()) +
  #geom_text(data = mutate_if(cor_data_sum, is.numeric, round, 2),
   # aes(label = glue("{value} [{.lower}, {.upper}]"), x= Inf), hjust = "inward")+
  theme_minimal()+
  facet_grid(~phase)+
  xlim(-0.5,1)+
  guides(fill = F)+
  scale_fill_colorblind()

rel_plot_dis <- cor_data%>%
  mutate(time_span = abs(as.numeric(term) - as.numeric(time_point)))%>%
  ggplot(aes(x = time_span, y = value, col = task))+
  geom_point(alpha = .5, size = 0.75)+
  geom_smooth(method = "lm", se = F, col = "black", lty = 2, size = .5)+
  stat_cor(method = "pearson", aes(x = time_span, y = value, label = paste(..r.label..)), inherit.aes = F,r.accuracy = 0.01, cor.coef.name = "r", label.x = 1, label.y = 0.99,)+
  ylim(0,1)+
  labs(y = "Correlation Coefficient",
     x = "Time Span") +
  facet_grid(phase~task)+
  theme_minimal()+
  guides (col = F)+
  scale_colour_colorblind()

```

```{r relplot, out.width="100%", fig.height=6, set.cap.width=T, num.cols.cap=2, fig.cap = "(A) Distribution of correlations between time points for each task. Dots represent the mean of the distribution with 95\\% HDI. Numbers denote mean and 95\\% HDI. (B) Correlations between re-test reliability and time span (in time points) between the testing time points."}
ggarrange(rel_plot, rel_plot_dis, heights = c(1,1.5), labels = c("A","B"), nrow = 2)
```

```{r, include=F}
# latent state models
lsc <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LS_cau.out")
lsi <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LS_inf.out")
lsq <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LS_quant.out")
lsg <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LS_gaze.out")

# latent state trait models
lstc <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_cau_equSvar.out")
lsti <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_inf_equSvar.out")
lstq <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_quant_equSvar.out")
lstg <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_gaze_equSvar.out")

# latent state trait models with auto regressive component
lstarc <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_cau_equSvar_AR1.out")
lstari <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_inf_equSvar_AR1.out")
lstarq <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_quant_equSvar_AR1.out")
lstarg <- readModels("../../models and documentation Jana/Application/one task models/test_apes_data_LST_gaze_equSvar_AR1.out")
```

## Stability and Reliability

As mentioned above, we fit three different SEMs to the data from each task. Each model offers a slightly different perspective on how stable and reliable performance is. We report the results starting with the LS model, followed by the LST model and finally the LST-AR model.

Reliability is defined as the proportion of true-score variance relative to the total (observed) variance, that is, in the context of the specified SEMs, variance that is explained by the latent state (residual) and trait variables. Reliability is estimated based on the correlations between indicators. Because the two indicators corresponded to the two test halves in our case, reliability was equivalent to a split-half reliability estimate.

In the LS models, we can look at the stability of group level performance by comparing the latent means estimated for each time point to see if they differ substantially from one another. To assess the stability of individual differences, we can look at the correlations between the latent state estimates for the different time points. 

For LST models, we can assess stability of individual differences by looking at consistency and occasion specificity estimates. A high level of consistency means that a large portion of the variation observed in performance at the different time points can be traced back to variation in the overarching trait. High levels of occasion specificity mean the inverse, namely that large portions of the variation in performance is explained by variation in the state residual -- that is, occasion-specific variation not explained by the trait, due to situation and / or person-situation interaction effects. 

LST-AR models extend LST models by accounting for potential carry-over effects between temporally adjacent observations. The autoregressive effects thereby capture accumulated situational effects, that is, effects of previously experienced situations on subsequent trait levels [@eid2017definition]. For instance, a person that experiences a great success at work at time $t$ may not only show greater job satisfaction as compared to their habitual trait level at that point in time, but an increase in job satisfaction that lasts across a prolonged time period (i.e., change in the habitual trait level at time $t'$ with $t' > t$ due to the positive experience at time $t$). LST-AR models allow us to quantify the temporal predictability of performance based on occasion-specific variance in the previous time points. This is captured in the (autoregressive) predictability coefficient and quantifies how much of the variation in performance can be explained by the variation in the occasion-specific variables at the previous time point (note that the names of the coefficients used here slightly differ from those used in @eid2017definition). 

We ran the same models for the data from Phase 1 and Phase 2. We first report the results for each task separately for the two phases and then compare how they differ between phases.

### Phase 1

To get an overview of the results, we first visualized the data. Figure \@ref(fig:perfplot) shows performance at the different time points. From a group-level perspective, we can say that performance was consistently above chance (0.5) in the causal inference and quantity tasks. For gaze following, there is no meaningful chance level. We can note, however, that group level performance never went down to zero, which would be expected if apes did not pay attention to the experimenter's gaze. The performance score in the switching task was largely negative, suggesting no successful switching between the two phases. 

For a first glimpse on the stability of individual differences, we correlated performance at the different time points for each task (all possible combinations of time points). Figure \@ref(fig:relplot)A visualizes the distribution of raw correlations between the different time points and \@ref(fig:relplot)B shows the relation between re-test correlations and the time span between time points. Correlations between time points were large and clearly different from zero for quantity, inference and gaze following. For quantity, this distribution was wider and closer to zero, but still clearly positive. For switching, the distribution was even wider and substantially overlapped with zero. For all tasks, correlations between time points tended to be lower for time points that were further apart [@uher2011individual]. 

We excluded the switching task from further analysis for two reasons. First, group level scores were constantly negative and performance in the feature trials always overlapped with chance. This suggests that, as a group, apes did not successfully switch strategies (see Figure \@ref(fig:perfplot)). Second, the correlations between the different measurement time points were low, suggesting no systematic individual differences (see Figure \@ref(fig:relplot)).

Next, we report the SEM results for the different tasks and the relations between them. All models showed acceptable fit indices (see Table \@ref(tab:semt)). The threshold parameters for each model are shown in (see Table \@ref(tab:thresht)).

```{r}
mot <- bind_rows(
  tibble(
  Task = c("causality", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsc$summaries$PostPred_PValue,lstc$summaries$PostPred_PValue,lstarc$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsc$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsc$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstc$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsc$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstarc$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstarc$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
  tibble(
  Task = c("inference", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsi$summaries$PostPred_PValue,lsti$summaries$PostPred_PValue,lstari$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsi$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsi$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lsti$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsi$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstari$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstari$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
  tibble(
  Task = c("gaze_following", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsg$summaries$PostPred_PValue,lstg$summaries$PostPred_PValue,lstarg$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsg$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsg$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstg$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsg$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstarg$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstarg$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
  tibble(
  Task = c("quantitiy", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsq$summaries$PostPred_PValue,lstq$summaries$PostPred_PValue,lstarq$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsq$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsq$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstq$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsq$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstarq$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstarq$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")))
)

```

```{r semt}
kable(mot, align = c("l", "l", "c", "c"), caption = "Model fit indices phase 1", booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("LSM = Latent state model", "LSTM = Latent state-trait model", "LSTM-AR = LST model with autoregressive component", "PPP = Posterior predictive p-value", "Chi 95% CI = 95%CI of difference between predicted and observed chi-square values"))
```

```{r}
thresh <- bind_rows(
  bind_rows(
    lsc$parameters$unstandardized%>%mutate(Model = "LSM", Task = "causality"),
    lstc$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstarc$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsi$parameters$unstandardized%>%mutate(Model = "LSM", Task = "inference"),
    lsti$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstari$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsg$parameters$unstandardized%>%mutate(Model = "LSM", Task = "gaze_following"),
    lstg$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstarg$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsq$parameters$unstandardized%>%mutate(Model = "LSM", Task = "quantitiy"),
    lstq$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstarq$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est)
)


```

```{r thresht}
kable(thresh, caption = "Threshold parameters phase 1", align = c("l", "l", "c", "c", "c", "c", "c", "c"), booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("LSM = Latent state model", "LSTM = Latent state-trait model", "LSTM-AR = LST model with autoregressive component", "T1-6 = Threshold parameters for response categories"))
```

#### Causal inference

To fit the models, the response categories of 0 or 1 solved trial had to be collapsed into one category due to sparsity. Furthermore, the thresholds could not be set equal for test-half 2 at time point 3 and 11 as well as test-half 1 at time points 4 and 12 due to a different number of observed categories for the respective test halves and time point combination. Latent means can still be compared across time for the state factors based on the respective other test half. At time point 7, thresholds of both test-halves could not be set invariant across time (due to a divergent number of observed categories). Latent mean differences for the latent state variable at time point 7 should therefore be interpreted with caution.

Figure \@ref(fig:lseplot) visualizes the latent state means and reliability estimates from the LS model. Reliability was consistently high. None of the latent means was significantly different from zero, suggesting stable group level performance and no systematic mean change over time. Figure \@ref(fig:lsplot) gives the correlations between the latent states for the different time points. Correlations were generally high, indicating stable individual differences.

```{r}
plse <- bind_rows(lsc$parameters$unstandardized%>%mutate(task = "causality"),
          lsi$parameters$unstandardized%>%mutate(task = "inference"),
          lsq$parameters$unstandardized%>%mutate(task = "quantity"),
          lsg$parameters$unstandardized%>%mutate(task = "gaze_following"))%>%
filter(paramHeader == "Means")%>%
  mutate(time_point = factor(as.numeric(gsub("[^0-9.-]", "", param))),
         estimate = "Latent state mean",
         ref = 0)

plsr <- bind_rows(lsc$parameters$r2%>%mutate(task = "causality"),
          lsi$parameters$r2%>%mutate(task = "inference"),
          lsq$parameters$r2%>%mutate(task = "quantity"),
          lsg$parameters$r2%>%mutate(task = "gaze_following"))%>%
  slice(which(row_number() %% 2 == 1))%>%
  mutate(time_point = factor(str_remove(param, "P1")),
         estimate = "Reliability",
         ref = NA)
  
plser <- 
  bind_rows(plse, plsr)%>%
  ggplot(aes(x = time_point, y = est, col = task))+
  facet_wrap_custom(estimate~., scales = "free_y", ncol = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(-5, 1))),
    scale_override(2, scale_y_continuous(limits = c(0, 1)))
  ))+
  geom_hline(aes(yintercept = ref), lty = 2, alpha = .75)+
  labs(x = "Time point",y = "Estimate")+
  geom_line(aes(group = task),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci),position = position_dodge(width = 0.2), alpha = .75)+
  scale_color_colorblind(name = "Task")+
  theme_minimal()
```


```{r lseplot, fig.height = 4, fig.cap = "Phase 1: latent means and reliability estimates with 95\\% CI for each time point based on LSM. Means at time point 1 are set to 0.", out.width="100%"}
plser
```

In the LSTM, the consistency coefficient was estimated to be around .903. This means that around 90% of true inter-individual differences are attributable to stable (trait) differences between individuals, while approximately 10% are due to variance in time point specific deviations from the stable trait. Reliability (across time points) was estimated to be high with a mean of .725 (see Figure \@ref(fig:lsteplot)).

Figure \@ref(fig:lstareplot) shows the parameters from the LSTM-AR for three time points (2, 3 and 14). Around 82.3% of true interindividual differences at time point 14 go back to stable trait differences, around 10.6% of true inter-individual differences can be explained by carry-over effects from previous time points (i.e. inertia in the within-person process) and only 6.1% of the variance is due to time-specific variation, that is, variance in the time specific deviation of true scores from the stable trait level that could not be predicted by the autoregressive process.

In sum, all models converge on the conclusion that group- and individual-level performance was highly stable over time. As noted above -- and as can be seen in Figure \@ref(fig:perfplot) -- performance on a group level was clearly above chance.  

```{r}
cor <- bind_rows(
  lsc$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "causality"),
  lsi$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "inference"),
  lsq$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "quantity"),
  lsg$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "gaze_following")
)%>%
  mutate(paramHeader = str_replace(paramHeader, "\\.WITH",""),
         paramHeader = factor(as.numeric(str_replace(paramHeader, "S",""))),
         param = factor(as.numeric(str_replace(param, "S",""))))

plsc <- cor%>%
  ggplot(aes(y = paramHeader, x = param, fill = est))+
    geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "#FFE066", 
   midpoint = 0.5, limit = c(0,1), space = "Lab", 
   name="Correlation")+
  geom_text(aes(label = round(est, 2)), size = 2)+
  scale_y_discrete(limits=rev, breaks = c(2,14))+
  scale_x_discrete(breaks = c(1,13))+
  facet_wrap(~task, ncol = 2)+
  theme_few()+
  theme(legend.position = c(0.9,0.85), legend.direction = "vertical", legend.title = element_blank(), legend.background = element_blank())
```

```{r lsplot, fig.height=6, fig.cap = "Phase 1: correlations between latent state variables based on LSM for the different tasks.", out.width="100%"}
plsc
```

#### Inference by exclusion

Thresholds could not be set equal for indicator 2 at time point 6 as well as indicator 1 at time points 7 and 14, due to a different number of observed categories for the respective indicator and time-point combination. Latent means can still be compared across time for the respective state factors based on the other indicator.

Reliability was high in the LS model and none of the latent means differed from zero (Figure \@ref(fig:lse)). Correlations between latent states were generally high across time points (Figure \@ref(fig:lsc)). 

In the LSTM, consistency was estimated to be around .859 -- around 86% of true inter-individual differences were attributable to stable (trait) differences between individuals. Approximately 14% were due to variance in time-point specific deviations from the stable trait. Reliability was high with an estimate of .815 (see Figure \@ref(fig:lsteplot)).

According to the LSTM-AR, around 79.4% of true inter-individual differences at time point 14 went back to stable trait differences and around 8.3% of true inter-individual differences can be explained by carry-over effects from previous time points. Around 11.3% of the variance was due to time-specific variance between individuals.

Taken together, we saw a similar pattern as for the causal inference task: Performance was very stable on a group level and so were the differences between individuals. Interestingly, from Figure \@ref(fig:perfplot) we take that group-level performance was at chance. The stable individual differences we found here suggest that variation around this mean was systematic and therefore that some individuals consistently performed above chance. Thus, despite the fact that this task was very difficult for apes, it was suitable to measure individual differences. 

```{r}
plste <- bind_rows(
  lstc$parameters$unstandardized%>%mutate(task = "causality"),
  lsti$parameters$unstandardized%>%mutate(task = "inference"),
  lstq$parameters$unstandardized%>%mutate(task = "quantity"),
  lstg$parameters$unstandardized%>%mutate(task = "gaze_following")
  )%>%
  filter(paramHeader== "New.Additional.Parameters",
         !grepl("DIF",param))%>%
  mutate(param = recode(param, CON1 = "Consistency", OCC1 = "Occasion specificity", REL1 = "Reliability"))%>%
  ggplot(aes(x = param, y = est, col = task))+
    geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci), position = position_dodge(width = .9))+
  theme_minimal()+
  ylim(0,1)+
  facet_grid(~param, scales = "free_x")+
  labs(x = "", y = "Model estiamte")+
  scale_x_discrete(limits=rev)+
  theme(axis.text.x = element_blank(), legend.position = "bottom")+
  scale_color_colorblind(name = "Task")
```

```{r lsteplot, fig.height = 2.5, fig.cap = "Phase 1: model parameters (with 95\\% CI) from LSTM for the four tasks.", out.width="100%"}
plste
```

```{r}
lstare<- bind_rows(
  lstarc$parameters$unstandardized%>%mutate(task = "causality")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstari$parameters$unstandardized%>%mutate(task = "inference")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstarq$parameters$unstandardized%>%mutate(task = "quantity")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstarg$parameters$unstandardized%>%mutate(task = "gaze_following")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstarc$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "causality"),
  lstari$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "inference"),
  lstarq$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "quantity"),
  lstarg$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "gaze_following")
  )%>%
  filter(!grepl("DIF",param), 
         param != "CON1")%>%
  separate(param, 
           into = c("param", "time_point"), 
           sep = "(?<=[A-Za-z])(?=[0-9])"
           )%>%
  filter(time_point != "4")%>%
  mutate(time_point = as.numeric(time_point),
         param = recode(param, CON = "Consistency", OSP = "Occasion specificity", REL = "Reliability", PRED = "Predictability"))

plstare <- ggplot(lstare, aes(x = factor(time_point), y = est, col = task))+
    geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci), position = position_dodge(width = .2))+
  geom_line(aes(group = task), position = position_dodge(width = 0.2))+
  theme_minimal()+
  ylim(0,1)+
  labs(x = "Time point", y = "Estiamte")+
  facet_grid(~param)+
  scale_color_colorblind(name = "Task")+
  theme(legend.position = "bottom")
```


```{r lstareplot, fig.height = 2.5, fig.cap = "Phase 1: model parameters (with 95\\% CI) from LSTM-AR for the four tasks.", out.width="100%"}
plstare
```

#### Gaze following

For gaze following, we had only 8 observed trials per measurement occasion. The highest two categories (3 and 4 correctly solved trials) were collapsed into one category due to sparsity. Thresholds could not be set equal for test half 2 at time point 9 as well as test half 1 and test half 2 at time point 8, due to a different number of observed categories for the respective test half and time point combination. Latent means can still be compared across time with the exception of time point 8.

Latent state means estimated in the LSM varied between -0.990 and -2.153 (for time point 8 the latent state mean was -2.77, but, as mentioned above, thresholds for this time point were not invariant). All of the latent state means were significantly lower than zero, suggesting a decrease in gaze following after the second time point (Figure \@ref(fig:lseplot)). Reliability was high for all time points. The correlations between latent states for the different time points were generally high, pointing to stable individual differences (Figure \@ref(fig:lsplot)). 

In the LSTM, consistency was estimated to be around .758, that is around 76% of true inter-individual differences are attributable to stable differences between individuals. Approximately 24% of inter-individual differences were due to variance in time point specific deviations from the stable trait. Reliability was high with an estimate of .792.

According to the LSTM-AR, around 85.5% of true inter-individual differences at time point 14 went back to stable trait differences and around 3.3% of true inter-individual differences can be explained by carry-over effects from previous time points. Around 10.9% of the variance was due to time-specific variance between individuals. However, earlier time points showed a different pattern. Time point 2 had a significantly higher predictability, which declined thereafter. For consistency, this pattern was reversed (Figure \@ref(fig:lstareplot), with only 22.4% and 51,2% of the true score variance at time point 1 and 2, respectively, being explained by stable inter-individual trait differences. Additionally, results of the LS model show that a) latent state means (average gaze following) decline over the first few time points (Figure 11) and that b) correlations between latent states increase and stabilize after the first four time points. Taken together, this suggests a habituation effect of the sort that initially, more individuals followed gaze but this number quickly declined to a habitual level where only a few – but mostly the same – individuals followed gaze.

In sum, we see a change in gaze following over time (Figure \@ref(fig:perfplot)). This group-level effect, however, did not affect differences between individuals, which were systematic across time points. 

#### Quantity

The lowest three (out of seven possible) categories (0, 1 and 2 correctly solved trials per test half) were collapsed due to sparsity. Thresholds could not be set equal for test half 1 at time point 5, due to a different number of observed categories for the respective test half and time-point combination. Latent means can still be compared across time.

Latent state means estimated in the LSM varied very little and all lay between -0.058 and 0.369. None of these state means differed from zero (Figure \@ref(fig:lseplot)). Reliability estimates were substantially lower compared to the other tasks.

The consistency coefficient was estimated to be around .964, that is around 96% of true inter-individual differences was attributable to stable differences between individuals and only approximately 3.6% were due to variance in time-point specific deviations from the stable trait. Again, reliability was rather low with an estimate of .446.

According to the LSTM-AR, around 95.8% of true inter-individual differences at time point 14 went back to stable trait differences and only 0.7% of the inter-individual differences can be explained by carry-over effects from previous time points. The remaining 2.9% of the variance was due to time-specific variance between individuals.

Taken together, quantity judgments were very stable over time on the group level (see also Figure \@ref(fig:perfplot)). The low reliability estimates suggest, however, that the task is less suited to capture individual differences.

### Phase 2

```{r, include=F}
# latent state models
lsc2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LS_cau.out")
lsi2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LS_inf.out")
lsq2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LS_quant.out")
lsg2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LS_gaze.out")
lsd2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LS_delay.out")

# latent state trait models
lstc2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_cau_equSvar.out")
lsti2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_inf_equSvar.out")
lstq2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_quant_equSvar.out")
lstg2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_gaze_equSvar.out")
lstd2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_delay_equSvar.out")

# latent state trait models with auto regressive component
lstarc2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_cau_equSvar_AR1.out")
lstari2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_inf_equSvar_AR1.out")
lstarq2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_quant_equSvar_AR1.out")
lstarg2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_gaze_equSvar_AR1.out")
lstard2 <- readModels("../../models and documentation Jana/Application/Testphase 2/one construct models/test_apes_data_LST_delay_equSvar_AR1.out")
```

For a visual overview of group-level performance in the different tasks see Figure \@ref(fig:perfplot). The pattern observed in Phase 1 was similar in Phase 2 for causal inference and quantity. The rate of gaze following remained on the low level reached at the end of Phase 1. For inference, there was an increase in performance over time, which already began to show itself at the end of Phase 1. Taken together, there was a lot of continuity between Phase 1 and 2. The newly added delay of gratification task produced fairly variable results -- with no ceiling or floor effect.

Correlations between time points were similar in Phase 1 and 2 for all tasks (Figure \@ref(fig:relplot)). Performance in the delay of gratification task proofed to be stable between individuals. Next, we present the results from the SEMs separate for each task. All models showed acceptable fit indices, except for the inference LST and LST-AR models (see Table \@ref(tab:semt2)). The threshold parameters for each model are shown in (see Table \@ref(tab:thresht2)).

```{r}
mot2 <- bind_rows(
  tibble(
  Task = c("causality", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsc2$summaries$PostPred_PValue,lstc2$summaries$PostPred_PValue,lstarc2$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsc2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsc2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstc2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsc2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstarc2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstarc2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
  tibble(
  Task = c("inference", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsi2$summaries$PostPred_PValue,lsti2$summaries$PostPred_PValue,lstari2$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsi2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsi2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lsti2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsi2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstari2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstari2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
  tibble(
  Task = c("gaze_following", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsg2$summaries$PostPred_PValue,lstg2$summaries$PostPred_PValue,lstarg2$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsg2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsg2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstg2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsg2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstarg2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstarg2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
  tibble(
  Task = c("quantitiy", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsq2$summaries$PostPred_PValue,lstq2$summaries$PostPred_PValue,lstarq2$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsq2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsq2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstq2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsq2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstarq2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstarq2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "))),
    tibble(
  Task = c("delay of gratification", "", ""),
  Model = c("LSM","LSTM","LSTM-AR"),
  PPP = c(lsd2$summaries$PostPred_PValue,lstd2$summaries$PostPred_PValue,lstarq2$summaries$PostPred_PValue),
  `Chi 95% CI` = c(paste(lsd2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsd2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstd2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lsd2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
                           paste(lstard2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstard2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")))
)

```

```{r semt2}
kable(mot2, align = c("l", "l", "c", "c"), caption = "Model fit indices phase 2", booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("LSM = Latent state model", "LSTM = Latent state-trait model", "LSTM-AR = LST model with autoregressive component", "PPP = Posterior predictive p-value", "Chi 95% CI = 95%CI of difference between predicted and observed chi-square values"))
```

```{r}
thresh2 <- bind_rows(
  bind_rows(
    lsc2$parameters$unstandardized%>%mutate(Model = "LSM", Task = "causality"),
    lstc2$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstarc2$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsi2$parameters$unstandardized%>%mutate(Model = "LSM", Task = "inference"),
    lsti2$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstari2$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsg2$parameters$unstandardized%>%mutate(Model = "LSM", Task = "gaze_following"),
    lstg2$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstarg2$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsq2$parameters$unstandardized%>%mutate(Model = "LSM", Task = "quantitiy"),
    lstq2$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstarq2$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est),
  
    bind_rows(
    lsd2$parameters$unstandardized%>%mutate(Model = "LSM", Task = "delay_of_gratification"),
    lstd2$parameters$unstandardized%>%mutate(Model = "LSTM", Task = ""),
    lstard2$parameters$unstandardized%>%mutate(Model = "LSTM-AR", Task = ""))%>%
    filter(paramHeader=="Thresholds")%>%
    select(Task, Model, param, est)%>%
    filter(grepl("P11\\$",param))%>%
    separate(param,into = c("tp","Threshold"), sep = "\\$")%>%
    mutate(Threshold = paste("T", Threshold, sep = ""))%>%
    select(-tp)%>%
    pivot_wider(names_from = Threshold, values_from = est)
)


```

```{r thresht2}
kable(thresh2, caption = "Threshold parameters phase 2", align = c("l", "l", "c", "c", "c", "c", "c", "c"), booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("LSM = Latent state model", "LSTM = Latent state-trait model", "LSTM-AR = LST model with autoregressive component", "T1-6 = Threshold parameters for response categories"))
```

#### Causal inference

... model fitting stuff ...

Figure \@ref(fig:lseplot2) visualizes the latent state means and reliability estimates from the LS model. Reliability was consistently high. Except for time point 2, none of the latent means was significantly different from zero, suggesting stable group level performance and no systematic mean change over time. Figure \@ref(fig:lsplot2) gives the correlations between the latent states for the different time points. Correlations were generally high -- indicating stable individual differences -- with a trend towards lower correlations for time points further apart. 

In the LSTM, the consistency coefficient was estimated to be around .912. This means that around 91% of true inter-individual differences are attributable to stable (trait) differences between individuals, while approximately 9% are due to variance in time point specific deviations from the stable trait. Reliability (across time points) was estimated to be high with a mean of .726 (see Figure \@ref(fig:lsteplot2)).

Figure \@ref(fig:lstareplot2) shows the parameters from the LSTM-AR for three time points (2, 3 and 14). It shows that consistency increased while predictability decreased over time. At time point 2, 54% of true interindividual differences go back to stable trait differences (time point 14: 69%) while around 43% of true inter-individual differences can be explained by carry-over effects from previous time points (time point 14: 27%). The variance due to time-specific variation was consistently low.

In sum, the different models suggest that group- and individual-level performance was relatively stable over time. As noted above -- and as can be seen in Figure \@ref(fig:perfplot) -- performance on a group level was clearly above chance.  

```{r}
plse2 <- bind_rows(lsc2$parameters$unstandardized%>%mutate(task = "causality"),
          lsi2$parameters$unstandardized%>%mutate(task = "inference"),
          lsq2$parameters$unstandardized%>%mutate(task = "quantity"),
          lsg2$parameters$unstandardized%>%mutate(task = "gaze_following"),
          lsd2$parameters$unstandardized%>%mutate(task = "delay_of_gratification"))%>%
filter(paramHeader == "Means")%>%
  mutate(time_point = factor(as.numeric(gsub("[^0-9.-]", "", param))),
         estimate = "Latent state mean",
         ref = 0)%>%
  mutate(task = factor(task, levels = c("causality", "gaze_following", "inference", "quantity", "delay_of_gratification")))

plsr2 <- bind_rows(lsc2$parameters$r2%>%mutate(task = "causality"),
          lsi2$parameters$r2%>%mutate(task = "inference"),
          lsq2$parameters$r2%>%mutate(task = "quantity"),
          lsg2$parameters$r2%>%mutate(task = "gaze_following"),
          lsd2$parameters$r2%>%mutate(task = "delay_of_gratification"))%>%
  slice(which(row_number() %% 2 == 1))%>%
  mutate(time_point = factor(str_remove(param, "P1")),
         estimate = "Reliability",
         ref = NA)%>%
  mutate(task = factor(task, levels = c("causality", "gaze_following", "inference", "quantity", "delay_of_gratification")))
  
plser2 <- 
  bind_rows(plse2, plsr2)%>%
  ggplot(aes(x = time_point, y = est, col = task))+
  facet_wrap_custom(estimate~., scales = "free_y", ncol = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(-5, 1))),
    scale_override(2, scale_y_continuous(limits = c(0, 1)))
  ))+
  geom_hline(aes(yintercept = ref), lty = 2, alpha = .75)+
  labs(x = "Time point",y = "Estimate")+
  geom_line(aes(group = task),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci),position = position_dodge(width = 0.2), alpha = .75)+
  scale_color_colorblind(name = "Task")+
  theme_minimal()
```


```{r lseplot2, fig.height = 4, fig.cap = "Phase 2: latent means and reliability estimates with 95\\% CI for each time point based on LSM. Means at time point 1 are set to 0.", out.width="100%"}
plser2
```

```{r}
cor2 <- bind_rows(
  lsc2$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "causality"),
  lsi2$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "inference"),
  lsq2$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "quantity"),
  lsg2$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "gaze_following"),
    lsd2$parameters$stdyx.standardized%>%filter(grepl("WITH",paramHeader))%>%mutate(task = "delay_of_gratification")
)%>%
  mutate(paramHeader = str_replace(paramHeader, "\\.WITH",""),
         paramHeader = factor(as.numeric(str_replace(paramHeader, "S",""))),
         param = factor(as.numeric(str_replace(param, "S",""))))

plsc2 <- cor2%>%
  ggplot(aes(y = paramHeader, x = param, fill = est))+
    geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "#FFE066", 
   midpoint = 0.5, limit = c(0,1), space = "Lab", 
   name="Correlation")+
  geom_text(aes(label = round(est, 2)), size = 2)+
  scale_y_discrete(limits=rev, breaks = c(2,14))+
  scale_x_discrete(breaks = c(1,13))+
  facet_wrap(~task, ncol = 2)+
  theme_few()+
  theme(legend.position = c(0.7,0.15), legend.direction = "horizontal", legend.title = element_blank(), legend.background = element_blank())
```

```{r lsplot2, fig.height=6, fig.cap = "Phase 2: correlations between latent state variables based on LSM for the different tasks.", out.width="100%"}
plsc2
```




#### Inference by exclusion

... to be added after discussing issues with the models.

```{r}
plste2 <- bind_rows(
  lstc2$parameters$unstandardized%>%mutate(task = "causality"),
  lsti2$parameters$unstandardized%>%mutate(task = "inference"),
  lstq2$parameters$unstandardized%>%mutate(task = "quantity"),
  lstg2$parameters$unstandardized%>%mutate(task = "gaze_following"),
  lstd2$parameters$unstandardized%>%mutate(task = "delay_of_gratification")
  )%>%
  mutate(task = factor(task, levels = c("causality", "gaze_following", "inference", "quantity", "delay_of_gratification")))%>%
  filter(paramHeader== "New.Additional.Parameters",
         !grepl("DIF",param))%>%
  mutate(param = recode(param, CON1 = "Consistency", OCC1 = "Occasion specificity", REL1 = "Reliability"))%>%
  ggplot(aes(x = param, y = est, col = task))+
    geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci), position = position_dodge(width = .9))+
  theme_minimal()+
  ylim(0,1)+
  facet_grid(~param, scales = "free_x")+
  labs(x = "", y = "Model estiamte")+
  scale_x_discrete(limits=rev)+
  theme(axis.text.x = element_blank(), legend.position = "bottom")+
  scale_color_colorblind(name = "Task")
```

```{r lsteplot2, fig.height = 2.5, fig.cap = "Phase 2: model parameters (with 95\\% CI) from LSTM for the four tasks.", out.width="100%"}
plste2
```

```{r}
lstare2<- bind_rows(
  lstarc2$parameters$unstandardized%>%mutate(task = "causality")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstari2$parameters$unstandardized%>%mutate(task = "inference")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstarq2$parameters$unstandardized%>%mutate(task = "quantity")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstarg2$parameters$unstandardized%>%mutate(task = "gaze_following")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstard2$parameters$unstandardized%>%mutate(task = "delay_of_gratification")%>%filter(paramHeader== "New.Additional.Parameters"),
  lstarc2$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "causality"),
  lstari2$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "inference"),
  lstarq2$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "quantity"),
  lstarg2$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "gaze_following"),
  lstard2$parameters$r2%>%filter(grepl("P",param))%>%mutate(param = str_replace(param,"P1","REL"))%>%filter(param == "REL2"|param == "REL3" |param == "REL14")%>%mutate(task = "delay_of_gratification")
  )%>%
  filter(!grepl("DIF",param), 
         param != "CON1")%>%
  separate(param, 
           into = c("param", "time_point"), 
           sep = "(?<=[A-Za-z])(?=[0-9])"
           )%>%
  filter(time_point != "4")%>%
  mutate(time_point = as.numeric(time_point),
         param = recode(param, CON = "Consistency", OSP = "Occasion specificity", REL = "Reliability", PRED = "Predictability"))%>%
   mutate(task = factor(task, levels = c("causality", "gaze_following", "inference", "quantity", "delay_of_gratification")))

  plstare2 <- ggplot(lstare2,aes(x = factor(time_point), y = est, col = task))+
    geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci), position = position_dodge(width = .2))+
  geom_line(aes(group = task), position = position_dodge(width = 0.2))+
  theme_minimal()+
  ylim(0,1)+
  labs(x = "Time point", y = "Estiamte")+
  facet_grid(~param)+
  scale_color_colorblind(name = "Task")+
  theme(legend.position = "bottom")
```


```{r lstareplot2, fig.height = 2.5, fig.cap = "Phase 2: model parameters (with 95\\% CI) from LSTM-AR for the four tasks.", out.width="100%"}
plstare2
```

#### Gaze following

... model fitting stuff ...

None of the latent state means -- except for time point 14 -- were significantly different from zero, suggesting a stable level of gaze following in Phase 2 (Figure \@ref(fig:lseplot2)). Reliability was generally high, but variable, ranging between `r min(plsr2%>%filter(task == "gaze_following")%>%pull(est))` and `r max(plsr2%>%filter(task == "gaze_following")%>%pull(est))`. The correlations between latent states for the different time points were generally high, pointing to stable individual differences (Figure \@ref(fig:lsplot2)). 

In the LSTM, consistency was estimated to be around 0.769, that is around 77% of true inter-individual differences are attributable to stable differences between individuals. Approximately 23% of inter-individual differences were due to variance in time point specific deviations from the stable trait. Reliability was high with an estimate of 0.77.

In the LSTM-AR, the estimates for consistency, predictability and occasion specificity were stable over time. Around 73% of true inter-individual differences at time point 14 went back to stable trait differences and around 4% of true inter-individual differences can be explained by carry-over effects from previous time points. Around 21% of the variance was due to time-specific variance between individuals (Figure \@ref(fig:lstareplot2). In sum, gaze following in Phase 2 was stable over time with systematic and stable differences between individuals (Figure \@ref(fig:perfplot)).

#### Quantity

... model fitting stuff ...

Like for gaze following, none of the latent state means differed from zero, except for time point 14. Reliability estimates were substantially lower compared to the other tasks (Figure \@ref(fig:lseplot)). Correlations between time points were generally high, but somewhat lower compared to gaze following or causal inference. 

in the LST model, The consistency coefficient was estimated to be 0.939, that is around 94% of true inter-individual differences was attributable to stable differences between individuals. Only 6% inter-indivual differences were due to variance in time-point specific deviations from the stable trait. Reliability was once again low with an estimate of 0.436.

The LSTM-AR showed stable estimates for consistency, predictability and occasion specificity. In general, consistency and predictability estimates were close to one another. At time point 2, around 50% of true inter-individual differences went back to stable trait differences (time point 14: 48%) while 43% could be explained by carry-over effects from previous time points (time point 14: 45%). The remaining 7% of the variance was due to time-specific variance between individuals (same for time point 14).

Taken together, quantity judgments were stable over time on the group level but less so on an individual level. Group-level performance was also cleary different from chance (Figure \@ref(fig:perfplot)).

#### Delay of gratification

... model fitting stuff ...

Three of the latent state means were significantly different from zero (time point 4, 5 and 8). However, there was no clear temporal pattern in that the latent means steadily de- or increased (Figure \@ref(fig:lsplot2)). Reliability was very high and stable. The correlations between latent means for the different time points were generally high, with time point 4 being a notable exception with lower correlations with time points further away (Figure \@ref(fig:lsplot2)). 

In the LSTM, consistency was estimated to be around 0.772, that is around 77% of true inter-individual differences are attributable to stable differences between individuals. Approximately 23% of inter-individual differences were due to variance in time point specific deviations from the stable trait. Reliability was very high with an estimate of 0.891.

In the LSTM-AR, the estimates for consistency, predictability and occasion specificity were stable over time. Around 73% of true inter-individual differences at time point 14 went back to stable trait differences and around 5% of true inter-individual differences can be explained by carry-over effects from previous time points. Around 21% of the variance was due to time-specific variance between individuals (Figure \@ref(fig:lstareplot2)). 

In sum, delay of gratification showed some variation in group level performance. However, differences between individuals were highly stable over time (Figure \@ref(fig:perfplot)).

### Comparison between phases

For the comparison between the two phases, we will focus on the four tasks that were included in both: causal inference, gaze following, inference by exclusion and quantity. 

For the causal inference task, the raw performance in Figure \@ref(fig:perfplot) shows no marked difference between the two phases. Similarly, we see no systematic difference between the two phases when looking at the latent state means estimated by the LSM (Figure \@ref(fig:lsdifplot)). This suggests stable group-level performance across the two phases. The LSTM estimates for consistency, occasion specificity and reliability were also very similar for the two phases (Figure \@ref(fig:lstdifplot)), which indicates that individual differences were structured similarly in the two phases. We see a more pronounced difference in the results of the LST-AR model in that consistency is considerably lower while predictability is higher in Phase 2 compared to 1. The reason ... 


```{r}
plsdif <- bind_rows(
  plse%>%mutate(phase = "Phase 1"),
  plse2%>%mutate(phase = "Phase 2")
)%>%
  mutate(time_point = as.numeric(time_point))%>%
  filter(task != "delay_of_gratification")%>%
  ggplot(aes(x = time_point, y = est, col = phase))+
  facet_wrap(~task, nrow = 1)+
  geom_hline(aes(yintercept = ref), lty = 2, alpha = .75)+
  labs(x = "Time point",y = "Latent state mean")+
  geom_line(aes(group = phase),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci),position = position_dodge(width = 0.2), alpha = .75, size = .25)+
  scale_x_continuous(breaks = c(1,14))+
  scale_color_discrete(name = "Phase")+
  theme_few()
```

```{r lsdifplot, fig.height = 2.5, fig.cap = "Difference between model LSTM estimates for phase 1 and phase 2.", out.width="100%"}
plsdif
```

The gaze following task showed notable differences between the two phases. The initial decline in gaze following in the beginning of Phase 1 did not repeat itself in Phase 2. Performance in Phase 2 seemed to pick up where it left off at the end of Phase 1 (Figure \@ref(fig:perfplot)). The LSM mean estimates for the different time points follow a similar pattern (Figure \@ref(fig:lsdifplot)). Despite this difference in group-level variability, the near-identical LSTM estimates for consistency and occasion specificity showed that individual differences were structured in a similar way across the two phases (Figure \@ref(fig:lstdifplot)). In contrast to the other tasks, consistency and predictability -- as estimated by the LST-AR model -- were similar to the estimates of Phase 1.  

```{r}
plstdif <- bind_rows(
bind_rows(
  lstc$parameters$unstandardized%>%mutate(task = "causality"),
  lsti$parameters$unstandardized%>%mutate(task = "inference"),
  lstq$parameters$unstandardized%>%mutate(task = "quantity"),
  lstg$parameters$unstandardized%>%mutate(task = "gaze_following")
  )%>%
  filter(paramHeader== "New.Additional.Parameters",
         !grepl("DIF",param))%>%
  mutate(param = recode(param, CON1 = "Consistency", OCC1 = "Occasion specificity", REL1 = "Reliability"))%>%mutate(phase = "Phase 1"),
bind_rows(
  lstc2$parameters$unstandardized%>%mutate(task = "causality"),
  lsti2$parameters$unstandardized%>%mutate(task = "inference"),
  lstq2$parameters$unstandardized%>%mutate(task = "quantity"),
  lstg2$parameters$unstandardized%>%mutate(task = "gaze_following")
  )%>%
  filter(paramHeader== "New.Additional.Parameters",
         !grepl("DIF",param))%>%
  mutate(param = recode(param, CON1 = "Consistency", OCC1 = "Occasion specificity", REL1 = "Reliability"))%>%mutate(phase = "Phase 2")
)%>%
  select(phase,param, task,est)%>%
  pivot_wider(names_from = phase, values_from = est)%>%
  mutate(dif = `Phase 1` - `Phase 2`)%>%
  ggplot(aes(x = task, y = dif, col = task))+
  geom_hline(yintercept = 0, lty = 2, alpha = .75)+
  geom_segment(aes(x = task, y = 0, xend = task, yend = dif), alpha = .5, col = "black")+
  facet_grid(~param, scales = "free_x")+
  geom_point(size = 2)+
  ylim(-0.25, 0.25)+
  theme_few()+
  labs(x = "", y = "Difference in model \nestiamte btw. phases")+
  scale_x_discrete(limits=rev)+
  theme(axis.text.x = element_blank(), legend.position = "right")+
  scale_color_colorblind(name = "Task")
  
```

```{r lstdifplot, fig.height = 2, fig.cap = "Difference between model LSTM estimates for phase 1 and phase 2.", out.width="100%"}
plstdif
```

Inference by exclusion ...

Raw performance in the quantity task did not show a markedly different pattern in the two phases (Figure \@ref(fig:perfplot)). This was once again also reflected in the latent state means estimated by the LSM (Figure \@ref(fig:lsdifplot)) and suggests stable group-level performance across the two phases. The three LSTM estimates for consistency, occasion specificity and reliability were also very similar for the two phases, which indicates that individual differences were measured and structured in a similar way in the two phases (Figure \@ref(fig:lstdifplot)). In the LST-AR model we saw the now familiar pattern of lower consistency and higher predictability. The source of this difference is likely ...

```{r}
plstardif <- bind_rows(
  lstare%>%mutate(phase = "Phase 1"),
  lstare2%>%mutate(phase = "Phase 2")
)%>%
  filter(task != "delay_of_gratification", 
         param != "Reliability")%>%
  ggplot(aes(x = factor(time_point), y = est, col = phase))+
  facet_grid(param~task)+
  labs(x = "Time point",y = "Estimate")+
  geom_line(aes(group = phase),position = position_dodge(width = 0.2), alpha = .75)+
  geom_pointrange(aes(ymin = lower_2.5ci, ymax = upper_2.5ci),position = position_dodge(width = 0.2), alpha = .75, size = .25)+
  scale_color_discrete(name = "Phase")+
  theme_few()

```

```{r lstardifplot, fig.height = 4, fig.cap = "Difference between model LSTM-AR estimates for phase 1 and phase 2.", out.width="100%"}
plstardif
```

## Relations between tasks

To analyse relations between different tasks (constructs), we estimated separate LST models, each modeling the relation between two tasks. In these combined models, the sub-models for each task were equivalent to the LST models described above. For ease of model specification, the LST models were estimated as multilevel models. These models are equivalent to the LST models for single tasks under the assumption of strict factorial invariance. Figure \@ref(fig:relgraph) visualizes the model for two tasks. 

```{r relgraph, engine='tikz', fig.cap="Latent State-Trait models for two tasks with correlations between traits and states for two indicators and six measurement time points.", out.width="80%", fig.align='center'}

\usetikzlibrary{arrows} 
\usetikzlibrary{positioning} 
\usetikzlibrary{shapes} 
\usetikzlibrary{fit} 
\usetikzlibrary{backgrounds} 
\usetikzlibrary{calc}

\begin{tikzpicture}
 [manifest/.style={rectangle,draw=black!80,semithick,minimum width=1.5cm,inner sep=
4pt,text centered},
        latent/.style={ellipse,draw=black!80,thick,inner sep=2,text centered},
        on/.style={->,>=stealth',semithick},
        from/.style={<-,>=stealth',semithick},
        with1/.style={<->,>=stealth',semithick,bend left=30},
        with2/.style={<->,>=stealth',semithick,bend right=30},
        with3/.style={<->,>=stealth',semithick,bend left=50},]


%%%%%%%%%%%%% Konstrukt 1
   
% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o1) at (0,0) {$\zeta_{21}$};
                \node[latent] (o2) [below=of o1] {$\zeta_{22}$};
                \node[latent] (o3) [below=of o2] {$\zeta_{23}$};
                \node[latent] (o4) [below=of o3] {$\zeta_{24}$};
                \node[latent] (o5) [below=of o4] {$\zeta_{25}$};
                \node[latent] (o6) [below=of o5] {$\zeta_{26}$};
\end{scope}
                     
% Trait % 
\node[latent,node distance=7] (t) [right=of  o4]  {$T_{2}$};
                                             	                        
% Manifest % 
 \foreach \a in {1,...,6} {\node (y1\a) [right = 2 of o\a] (yhelp\a) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b] (y1\b) {$Y^*_{12\b}$}
 	edge [from] node[above] {1} (o\b);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c] (y2\c) {$Y^*_{22\c}$}
 	edge [from] node[below] {$1$} (o\c);} 

    
% Pfeile %
\foreach \a in {1,...,6}
	\path (y1\a.0) edge [from] (t);
\foreach \a in {1,...,6}	
	\path (y2\a.0) edge [from] (t);
    
  % Fehler %
\foreach \a in {1,...,5} {\node (e1\a) [below right = .3 of y1\a] {}
	edge [on] (y1\a.south east);}
\foreach \b in {1,...,5} {\node (e2\b) [below right = .3 of y2\b] {}
	edge [on] (y2\b.south east);}

\node (e16) [below right = .3 of y16] {$\epsilon_{126}$}
	edge [on] (y16.south east);
\node (e26) [below right = .3 of y26] {$\epsilon_{226}$}
	edge [on] (y26.south east);
	
	
  
  %%%%%%%%%%%%% Konstrukt 2
   
% Latent Variables %
 \begin{scope}[node distance=1.5]
                \node[latent] (o11) [left= 2 of o1] {$\zeta_{11}$};
                \node[latent] (o21) [below=of o11] {$\zeta_{12}$};
                \node[latent] (o31) [below=of o21] {$\zeta_{13}$};
                \node[latent] (o41) [below=of o31] {$\zeta_{14}$};
                \node[latent] (o51) [below=of o41] {$\zeta_{15}$};
                \node[latent] (o61) [below=of o51] {$\zeta_{16}$};
\end{scope}

% Manifest % 
 \foreach \a in {1,...,6} {\node (y11\a) [left = 2 of o\a1] (yhelp\a1) {};}        
 \foreach \b in {1,...,6} {\node[manifest] [above = .05 of yhelp\b1] (y11\b) {$Y^*_{11\b}$}
 	edge [from] node[above] {1} (o\b1);}
 \foreach \c in {1,...,6} {\node[manifest] [below = .05 of yhelp\c1] (y21\c) {$Y^*_{21\c}$}
 	edge [from] node[below] {$1$} (o\c1);} 

%Trait%
\node[latent,node distance=7] (t1) [left=of  o41]  {$T_{1}$};

% Pfeile %
\foreach \a in {1,...,6}
	\path (y11\a.180) edge [from] (t1);
\foreach \a in {1,...,6}	
	\path (y21\a.180) edge [from] (t1);


  % Fehler %
\foreach \a in {1,...,5} {\node (e11\a) [below left = .3 of y11\a] {}
	edge [on] (y11\a.south west);}
\foreach \b in {1,...,5} {\node (e21\b) [below left = .3 of y21\b] {}
	edge [on] (y21\b.south west);}

\node (e16) [below left = .3 of y116] {$\epsilon_{116}$}
	edge [on] (y116.south west);
\node (e26) [below left = .3 of y216] {$\epsilon_{216}$}
	edge [on] (y216.south west);
	

%%%%%%%%%%%%% Korrelationen
\foreach \a in {1,...,6} {\draw[<->,>=stealth',semithick,] (o\a) to [with1] (o\a1);}

\coordinate [below = 6cm of t1] (fit above) {};
\coordinate [below= 6cm of t] (fit right) {};

\draw[<-,>=stealth',semithick] (t1.south) -- (fit above);
\draw[semithick] (fit above) -| (fit right);   
\draw[->,>=stealth',semithick] (fit right) to (t.south);  
   
\end{tikzpicture}

```

Detailed information on the parameter estimates obtained in LST models for each separate task is provided above. Here we report the results with a focus on the latent correlations only. The parameters of interest were correlations between a) the latent traits, indicating associations between stable cognitive ability as estimated by the different tasks, and b) correlations between state residual variables belonging to the same measurement time point, as an indicator of time-specific associations between latent abilities across the two tasks, above and beyond stable trait differences.

Simulation studies suggested that LST models in which latent correlations between time-specific state residual variables were estimated to be time-point specific (i.e. covariances and variances of state residual variables can freely vary across time) did not show good estimation performance under the given conditions (sample size, ordinal indicator variables, etc.). Therefore, we chose a model with fixed correlations between state residual variables across time. That is, a model in which we assumed that associations between latent time-specific cognitive abilities across two different tasks within each time point are equal at all time points. We think that this assumption is reasonable in the present context. As a consequence, just one correlation between latent state residual variables is estimated for each model. The corresponding model showed good estimation performance under the given sample sizes in a simulation study.

For details on MCMC estimation see section on [estimation](#estimation) above. Because the multi-construct models were considerably more complex (i.e. had more parameters), we increased the minimum number of iterations per Markov chain to 20,000 (with a thinning of 10, that is, 200,000 iterations per chain).

### Phase 1

```{r, include=F}
# latent state models
lstcg <- readModels("../../models and documentation Jana/Application/two tasks models/test_apes_data_LST_cau_gaze.out")
lstci <- readModels("../../models and documentation Jana/Application/two tasks models/test_apes_data_LST_cau_inf.out")
lstcq <- readModels("../../models and documentation Jana/Application/two tasks models/test_apes_data_LST_cau_quant_indspec.out")
lstig <- readModels("../../models and documentation Jana/Application/two tasks models/test_apes_data_LST_inf_gaze.out")
lstiq <- readModels("../../models and documentation Jana/Application/two tasks models/test_apes_data_LST_inf_quant.out")
lstqg <- readModels("../../models and documentation Jana/Application/two tasks models/test_apes_data_LST_quant_gaze.out")
```

```{r}
rtt <- tibble(
  Task1 = c("causality", "", "", "inference", "", "quantitiy"),
  Task2 = c("gaze following","inference","quantitiy", "gaze following","quantitiy","gaze following"),
  PPP = c(lstcg$summaries$PostPred_PValue,
          lstci$summaries$PostPred_PValue,
          lstcq$summaries$PostPred_PValue,
          lstig$summaries$PostPred_PValue,
          lstiq$summaries$PostPred_PValue,
          lstqg$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    paste(lstcg$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstcg$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstci$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstci$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstcq$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstcq$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstig$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstig$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstiq$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstiq$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstqg$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstqg$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")))
```

```{r relt}
kable(rtt, align = c("l", "l", "c", "c"), caption = "Model fit indices for multi-construct models phase 1", booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("PPP = Posterior predictive p-value", "Chi 95% CI = 95%CI of difference between predicted and observed chi-square values"))
```

Model fit indices are shown in Table \@ref(tab:relt). Due to a low PPP value, the model for causal inference and quantity was modified such that for each task, test-half specific trait factors were estimated on the between-level. The correlations between the two tasks are therefore also reported as test-half specific trait correlations. 

The only correlations for which the 95% CI did not include zero were the state residual correlation between causal inference and inference ($r_{sc,si}$ = `r lstci$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(est)`, 95% CI = [`r lstci$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(lower_2.5ci)`; `r lstci$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(upper_2.5ci)`]) and the trait correlation between inference and quantity ($r_{ti,tq}$ = `r lstiq$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(est)`, 95% CI = [`r lstiq$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(lower_2.5ci)`; `r lstiq$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(upper_2.5ci)`]). 

The negative state correlations between causal inference and inference may be explained by the way the two tasks were presented. Remember that causal inference and inference trials used the same setup and were intermixed. A negative correlation suggests that higher (residual) performance in one task was associated with lower performance in the other task. Responding correctly in the two tasks required opposite choice behaviors. That is, in causal inference, the ape had to pick the cup the experimenter shook to be correct. In inference, it was the unshaken cup. Such a negative correlation arises when sometimes participants respond in the same way (e.g. pick the shaken cup) across tasks. Note, however, that if this were a stable strategy which individuals would consistently use, we would have seen a negative correlation between the trait estimates. The best explanation is thus that there are short periods of inattentiveness during which (some) participants confused the two tasks.

The trait correlation between inference and quantity was positive, suggesting that individuals with better quantity judgment abilities also have better inferential abilities. 

One (out of four) of the test-half specific trait correlations between causal inference and quantity was also reliably different from zero ($r_{tc_2,tq_1}$ = `r lstcq$parameters$stdyx.standardized%>%filter(paramHeader == "TC2.WITH", param == "TQ1")%>%pull(est)`, 95% CI = [`r lstcq$parameters$stdyx.standardized%>%filter(paramHeader == "TC2.WITH", param == "TQ1")%>%pull(lower_2.5ci)`; `r lstcq$parameters$stdyx.standardized%>%filter(paramHeader == "TC2.WITH", param == "TQ1")%>%pull(upper_2.5ci)`]). We do not consider this result to be substantial evidence for a substantive association between the trait estimates in the two tasks and therefore do not interpret it any further. Figure \@ref(fig:mtmplot) shows all correlations between the different tasks.
```{r}
mtmcor <- bind_rows(
lstcg$parameters$stdyx.standardized,
lstci$parameters$stdyx.standardized,
lstcq$parameters$stdyx.standardized,
lstig$parameters$stdyx.standardized,
lstiq$parameters$stdyx.standardized,
lstqg$parameters$stdyx.standardized,
)%>%
  filter(grepl(".WITH", paramHeader))%>%
  mutate(paramHeader = str_replace(paramHeader, "\\.WITH",""),
         type = ifelse(grepl("S",paramHeader),"State residual","Trait"),
         task1 = str_remove_all(paramHeader,"[ST]"),
         task2 = str_remove_all(param,"[ST]"))%>%
  filter(!grepl("1", task1),!grepl("2", task1))%>%
  mutate(task1 = recode(task1, G = "gaze_following", I = "inference", C = "causality", Q = "quantity"),
         task2 = recode(task2, G = "gaze_following", I = "inference", C = "causality", Q = "quantity"),
         label = paste0(round(est,2)," \n [", round(lower_2.5ci,2)," ; ", round(upper_2.5ci,2),"]"))

pmtmcor <- mtmcor%>%
  ggplot(aes(y = task1, x = task2, fill = est, label = label))+
    geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "#FFE066", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation")+
  facet_grid(~type)+
  geom_text(size = 2.8, fontface = ifelse(mtmcor$sig == T, 2, 1))+
  theme_few()+
  scale_x_discrete(guide = guide_axis(n.dodge = 2),limits = c("causality", "quantity","gaze_following"))+
  scale_y_discrete(limits = c( "inference","gaze_following", "quantity"))+
  theme(legend.position = c(0.92,0.75), legend.direction = "vertical", legend.title = element_blank())
```

```{r mtmplot, fig.height=4, fig.cap = "Phase 1: correlations between latent traits and latent state residual variables, respectively, of different tasks. Bold correlations are different from zero as judged by the 95\\% CI. The trait correlations between quantity and causal inference are not displayed, see main text for details. See main text for details.", out.width="100%"}
pmtmcor
```

### Phase 2

```{r, include=F}
# latent state models
lstcg2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_cau_gaze.out")
lstci2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_cau_inf.out")
lstcq2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_cau_quant_indspec.out")
lstcd2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_cau_delay.out")

lstig2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_inf_gaze.out")
lstiq2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_inf_quant.out")
lstid2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_inf_delay.out")

lstqg2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_quant_gaze.out")
lstqd2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_quant_delay.out")

lstgd2 <- readModels("../../models and documentation Jana/Application/Testphase 2/two construct models/test_apes_data_LST_gaze_delay.out")

```

```{r}
rtt2 <- tibble(
  Task1 = c("causality", "", "","", "inference", "","", "quantitiy","","gaze following"),
  Task2 = c("gaze following","inference","quantitiy","delay of gratification", "gaze following","quantitiy","delay of gratification","gaze following","delay of gratification","delay of gratification"),
  PPP = c(lstcg2$summaries$PostPred_PValue,
          lstci2$summaries$PostPred_PValue,
          lstcq2$summaries$PostPred_PValue,
          lstcd2$summaries$PostPred_PValue,
          lstig2$summaries$PostPred_PValue,
          lstiq2$summaries$PostPred_PValue,
          lstid2$summaries$PostPred_PValue,
          lstqg2$summaries$PostPred_PValue,
          lstqd2$summaries$PostPred_PValue,
          lstgd2$summaries$PostPred_PValue),
  `Chi 95% CI` = c(
    paste(lstcg2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstcg2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstci2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstci2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstcq2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstcq2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstcd2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstcd2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(lstig2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstig2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstcq2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstcq2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstid2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstid2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
        
    paste(lstqg2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstqg2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    paste(lstqd2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstqd2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; "),
    
    paste(lstgd2$summaries$ObsRepChiSqDiff_95CI_LB%>%round(2)%>%format(nsmall =2), lstgd2$summaries$ObsRepChiSqDiff_95CI_UB%>%round(2)%>%format(nsmall =2), sep = " ; ")
    ))
```

```{r}
mtmcor2 <- bind_rows(
lstcg2$parameters$stdyx.standardized,
lstci2$parameters$stdyx.standardized,
lstcq2$parameters$stdyx.standardized,
lstcd2$parameters$stdyx.standardized,
lstig2$parameters$stdyx.standardized,
lstiq2$parameters$stdyx.standardized,
lstid2$parameters$stdyx.standardized,
lstqg2$parameters$stdyx.standardized,
lstqd2$parameters$stdyx.standardized,
lstgd2$parameters$stdyx.standardized,
)%>%
  filter(grepl(".WITH", paramHeader))%>%
  mutate(paramHeader = str_replace(paramHeader, "\\.WITH",""),
         type = ifelse(grepl("S",paramHeader),"State residual","Trait"),
         task1 = str_remove_all(paramHeader,"[ST]"),
         task2 = str_remove_all(param,"[ST]"))%>%
  filter(!grepl("1", task1),!grepl("2", task1))%>%
  mutate(task1 = recode(task1, G = "gaze_following", I = "inference", C = "causality", Q = "quantity", D = "delay_of_grat."),
         task2 = recode(task2, G = "gaze_following", I = "inference", C = "causality", Q = "quantity", D = "delay_of_grat."),
         label = paste0(round(est,2)," \n [", round(lower_2.5ci,2)," ; ", round(upper_2.5ci,2),"]"))
  

pmtmcor2 <- mtmcor2%>%
  ggplot(aes(y = task1, x = task2, fill = est, label = label))+
    geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "#FFE066", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
   name="Correlation")+
  facet_grid(~type)+
  geom_text(size = 2.2, fontface = ifelse(mtmcor2$sig == T, 2, 1))+
  theme_few()+
  scale_x_discrete(guide = guide_axis(n.dodge = 2),limits = c("causality", "quantity","gaze_following", "delay_of_grat."))+
  scale_y_discrete(limits = c( "inference","delay_of_grat.","gaze_following", "quantity"))+
  theme(legend.position = c(0.92,0.75), legend.direction = "vertical", legend.title = element_blank())
```

```{r relt2}
kable(rtt2, align = c("l", "l", "c", "c"), caption = "Model fit indices for multi-construct models phase 2", booktabs=TRUE)%>%
kable_classic(full_width = F)%>%
      footnote(c("PPP = Posterior predictive p-value", "Chi 95% CI = 95%CI of difference between predicted and observed chi-square values"))
```

Model fit indices are shown in Table \@ref(tab:relt2). Like in Phase 1, due to a low PPP value, the model for causal inference and quantity was modified such that for each task, test-half specific trait factors were estimated on the between-level. The correlations between the two tasks are therefore also reported as test-half specific trait correlations. 

For the state residuals, two correlations were estimated to have 95% CIs that did not include 0: inference by exclusion and quantity ($r_{si,sq}$ = `r lstiq2$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(est)`, 95% CI = [`r lstiq2$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(lower_2.5ci)`; `r lstiq2$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(upper_2.5ci)`]) as well as inference by exclusion and delay of gratification ($r_{si,sd}$ = `r lstid2$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(est)`, 95% CI = [`r lstid2$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(lower_2.5ci)`; `r lstid2$parameters$stdyx.standardized%>%filter(paramHeader == "SI.WITH")%>%pull(upper_2.5ci)`]). In both cases, the relation was positive, suggesting that time points on which an individual had a higher (or lower) proportion of variance not explained by the trait in one task, the same individual showed a similar result in the other task. Such a pattern arises when time point specific states (e.g., motivation, attentiveness) affect both tasks to a similar extent.

Five of nine trait correlations were significantly different from 0: quantity and inference by exclusion ($r_{ti,tq}$ = `r lstiq2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(est)`, 95% CI = [`r lstiq2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(lower_2.5ci)`; `r lstiq2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(upper_2.5ci)`]); quantity and delay of gratification ($r_{td,tq}$ = `r lstqd2$parameters$stdyx.standardized%>%filter(paramHeader == "TD.WITH")%>%pull(est)`, 95% CI = [`r lstqd2$parameters$stdyx.standardized%>%filter(paramHeader == "TD.WITH")%>%pull(lower_2.5ci)`; `r lstqd2$parameters$stdyx.standardized%>%filter(paramHeader == "TD.WITH")%>%pull(upper_2.5ci)`]); quantity and gaze following ($r_{tg,tq}$ = `r lstqg2$parameters$stdyx.standardized%>%filter(paramHeader == "TG.WITH")%>%pull(est)`, 95% CI = [`r lstqg2$parameters$stdyx.standardized%>%filter(paramHeader == "TG.WITH")%>%pull(lower_2.5ci)`; `r lstqg2$parameters$stdyx.standardized%>%filter(paramHeader == "TG.WITH")%>%pull(upper_2.5ci)`]); inference by exclusion and causal inference ($r_{tc,ti}$ = `r lstci2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(est)`, 95% CI = [`r lstci2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(lower_2.5ci)`; `r lstci2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(upper_2.5ci)`]); inference by exclusion and delay of gratification ($r_{ti,td}$ = `r lstid2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(est)`, 95% CI = [`r lstid2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(lower_2.5ci)`; `r lstid2$parameters$stdyx.standardized%>%filter(paramHeader == "TI.WITH")%>%pull(upper_2.5ci)`]). Thus, quantity and inference by exclusion were included in three of these models, delay of gratification in two and causal inference and gaze following in one. All of the correlations were positive, indicating that higher trait estimates in one task go together with higher trait estimates in the respective other. 

Two (out of four) of the test-half specific trait correlations between causal inference and quantity were also reliably different from zero ($r_{tc_1,tq_2}$ = `r lstcq2$parameters$stdyx.standardized%>%filter(paramHeader == "TC1.WITH", param == "TQ2")%>%pull(est)`, 95% CI = [`r lstcq2$parameters$stdyx.standardized%>%filter(paramHeader == "TC1.WITH", param == "TQ2")%>%pull(lower_2.5ci)`; `r lstcq2$parameters$stdyx.standardized%>%filter(paramHeader == "TC1.WITH", param == "TQ2")%>%pull(upper_2.5ci)`]; $r_{tc_2,tq_2}$ = `r lstcq2$parameters$stdyx.standardized%>%filter(paramHeader == "TC2.WITH", param == "TQ2")%>%pull(est)`, 95% CI = [`r lstcq2$parameters$stdyx.standardized%>%filter(paramHeader == "TC2.WITH", param == "TQ2")%>%pull(lower_2.5ci)`; `r lstcq2$parameters$stdyx.standardized%>%filter(paramHeader == "TC2.WITH", param == "TQ2")%>%pull(upper_2.5ci)`]). Given that a third correlation was also of similar magnitude, we consider this to be at least suggestive evidence that individuals with higher quantitative abilities were also better at making causal inferences. Figure \@ref(fig:mtmplot2) shows all correlations between the different tasks.

```{r mtmplot2, fig.height=4, fig.cap = "Phase 2: correlations between latent traits and latent state residual variables, respectively, of different tasks. Bold correlations are different from zero as judged by the 95\\% CI.", out.width="100%"}
pmtmcor2
```

### Comparison between phases

The most notable difference between the two phases was that we see more substantial trait correlations in Phase 2 compared to Phase 1. This was expected given that we had one additional task in Phase 2 and thus four more combined models. The only significant trait correlation from Phase 1 also turned out to be significant in Phase 2 (quantity and inference by exclusion). For the tasks that were part of both phases, correlations were numerically similar (Figure \@ref(fig:mtmplotc)).

These results suggest some commonalities between the traits -- with gaze following being an exception (see Figure \@ref(fig:mtmplot2)). However, the sample size we studied here is still comparatively small for individual differences research looking for correlations between traits. Larger samples are needed to corroborate these findings. 

The significant *negative* state residual correlation between causal inference and inference by exclusion found in Phase 1 was not present in Phase 2. This suggests a de-coupling between the two tasks in that apes were less likely to confuse them with one another. This interpretation is also supported by the increase in group-level performance for inference by exlucsion and an increase in the trait correaltion between the two tasks. 

```{r}
mtmcorc <- bind_rows(
  mtmcor%>%mutate(phase = "Phase 1"),
  mtmcor2%>%mutate(phase = "Phase 2"))%>%
  select(type, task1, task2,phase, est)%>%
  pivot_wider(names_from = "phase", values_from = "est")%>%
  mutate(dif = `Phase 1` - `Phase 2`)%>%
  filter(task1 != "delay_of_gratification", task2 != "delay_of_gratification")%>%
  pivot_longer(cols = c("Phase 1", "Phase 2"), names_to = "phase", values_to = "est")

pmtmcorc <- mtmcorc%>%
  ggplot(aes(y = task1, x = task2))+
   #geom_tile(color = "white", aes(fill = dif))+
  geom_point(aes(size = est, col = phase), alpha = .75, pch = 1, stroke = 1.5)+
  facet_grid(~type)+
  labs(x = "", y = "")+
  scale_color_discrete(name = "")+
  scale_size(name = "Correlation", limits = c(-.56, .5))+
  theme_few(base_size = 10)+
  scale_x_discrete(guide = guide_axis(n.dodge = 2),limits = c("causality", "quantity","gaze_following"))+
  scale_y_discrete(limits = c( "inference","gaze_following", "quantity"))+
  theme(legend.box = "horizontal")
```

```{r mtmplotc, fig.height=2.5, fig.cap = "Differences between phases in correlations between latent traits and latent state residual variables for the different tasks.", out.width="100%"}
pmtmcorc
```

## Predictability

The output of the projection predictive inference models is a ranking of the different predictors with respect to how much they improve a model's fit. Predictors ranked first improve the fit the most, while later predictors yield smaller improvements (if any). The selection of "relevant" predictors is based on plotting the loss statistics and looking for a point at which it levels off. As such, the selection is to some extend arbitrary. The ranking, however, is not. When we compare the results from the two phases, we not just look at which subset of predictors is selected, but also at the overall ranking.

### Phase 1

```{r}
#ncores <- parallel::detectCores()
```

```{r, include = FALSE}
# apes1_new <- read_csv("../../data/ppi_data_phase1.csv")%>%
#   mutate(across(c(subject, group, le_present, dist_present, sex, rearing, observer, test_day), as_factor)) %>%
#   mutate(observer = fct_relevel(observer, "no")) %>% 
#   jtools::center(.,vars = c("sick_severity",
#                             "le_mean",
#                             "time_outdoors",
#                             "age",
#                             "time_in_leipzig", 
#                             "test_tp")) %>%
#   group_by(group, time_point) %>%
#   mutate(rank_gmc = rank - mean(rank, na.rm = TRUE)) %>%
#   ungroup() %>%
#   arrange(time_point)
# 
# grp_size <- tibble(
#   # number of apes for each species
#   a_chimp = 20,
#   b_chimp = 6,
#   bonobo = 12,
#   gorilla = 6,
#   orangutan = 6
# )
# 
# apes1_new <- apes1_new %>%
#   # create rank variable depending on species
#   group_by(group, time_point) %>%
#   mutate(
#     rel_rank = case_when(
#       group == "a_chimp" ~ percent_rank(grp_size$a_chimp:1)[rank],
#       group == "b_chimp" ~ percent_rank(grp_size$b_chimp:1)[rank],
#       group == "bonobo" ~ percent_rank(grp_size$bonobo:1)[rank],
#       group == "gorilla" ~ percent_rank(grp_size$gorilla:1)[rank],
#       group == "orangutan" ~ percent_rank(grp_size$orangutan:1)[rank]
#     )
#   ) %>%
#   ungroup()
# 
# t_cau <- filter(apes1_new, task == "causality")
# t_inf <- filter(apes1_new, task == "inference")
# t_quant <- filter(apes1_new, task == "quantity")
# t_gaze <- filter(apes1_new, task == "gaze_following")
# 
# t_gaze <- t_gaze %>%
#   # create dummy variable indicating if session 1 or 2
#   group_by(time_point, session) %>% 
#   mutate(tp_mod = cur_group_id()) %>%
#   ungroup() %>% 
#   mutate(day2 = case_when(session == 1 ~ "no",
#                           session == 2 ~ "yes"),
#          day2 = factor(day2)) %>%
#   select(tp_mod, day2, everything())
# 
# t_gaze <- t_gaze %>%
#   # remove duplicates created by day2
#   group_by(subject) %>%
#   filter(!duplicated(tp_mod)) %>%   
#   ungroup() 
```


```{r, include = F}
# # covariate needed for projection prediction
# # placed here for easy editing of formula
# all_fixed_effects <- c("sick_severity",       
#                        "test_day", "test_tp", 
#                        "rel_rank",
#                        "observer", 
#                        "age", "time_in_leipzig", 
#                        "sex", "group",
#                        "rearing", 
#                        "le_mean", 
#                        "dist_mean",
#                        "time_outdoors", 
#                        "sociality")
# 
# fm <- formula(cogn ~ sick_severity +                    
#                 test_day + test_tp +                
#                 rel_rank + # rank_gmc +
#                 observer + 
#                 age + time_in_leipzig +
#                 sex + group +
#                 rearing +
#                 le_mean + # le_max + # le_present +         
#                 dist_mean + # dist_max + # + dist_present +
#                 time_outdoors +
#                 sociality + # sociality_total
#                 # heat_mod + # heat +
#                 (1|subject)                         
#               )
# 
# fm_gaze <- update(fm, . ~ . +day2)
```


```{r}
# m_cau_2l <- brm(fm, data = t_cau, 
#                 warmup = 2e3, iter = 4e3, cores = ncores, chains = 4, 
#                 seed = 2020,
#                 save_pars = save_pars(all = TRUE)
#                 )
# m_inf_2l <- brm(fm, data = t_inf, 
#                 warmup = 2e3, iter = 4e3, cores = ncores, chains = 4, 
#                 seed = 2020,
#                 save_pars = save_pars(all = TRUE)
#                 )
# m_quant_2l <- brm(fm, data = t_quant, 
#                   warmup = 2e3, iter = 4e3, cores = ncores, chains = 4, 
#                   seed = 2020,
#                   save_pars = save_pars(all = TRUE)
#                   )
# m_gaze_2l <- brm(fm_gaze, data = t_gaze, 
#                  warmup = 2e3, iter = 4e3, cores = ncores, chains = 4, 
#                  seed = 2020,
#                  save_pars = save_pars(all = TRUE)
#                  )
```

```{r, eval = F}
#lapply(list(m_cau_2l, m_inf_2l, m_quant_2l, m_gaze_2l), loo)
```

```{r}
# delay random intercept to last place so that it doesn't soak up all the variance
# s_terms <- c("1", all_fixed_effects,
#              paste0(paste(all_fixed_effects, collapse = " + "), " + (1 | subject)"))
# s_terms_gaze <- c("1", c(all_fixed_effects, "day2"),
#                   paste0(paste(c(all_fixed_effects, "day2"), collapse = " + "), " + (1 | subject)"))
```

```{r}
# refM_cau <- get_refmodel(m_cau_2l)
# refM_inf <- get_refmodel(m_inf_2l)
# refM_quant <- get_refmodel(m_quant_2l)
# refM_gaze <- get_refmodel(m_gaze_2l)
```

#### Causal inference

```{r, warning = FALSE}
# cvs_cau <- cv_varsel(refM_cau, 
#                      search_terms = s_terms, cv_method = "LOO", method = "forward", 
#                      seed = 2020)
# 
# saveRDS(cvs_cau,"./saves/cvs_cau.rds")
# 
# cvs_cau <- readRDS("./saves/cvs_cau.rds")
```

```{r}
# cvarselp <- summary(cvs_cau, stats = c('elpd', 'rmse'))%>%
#   tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(solution_terms = factor(ifelse(is.na(solution_terms),"none",solution_terms)),
#          select = ifelse(solution_terms  %in% c("(1 | subject)", "group"), "yes", "no"))
# 
# saveRDS(cvarselp, "./saves/cvs_cau_summary.rds")

cvarselp <- readRDS("./saves/cvs_cau_summary.rds")

pcsel <- cvarselp%>%
ggplot(aes(x = size, y = value))+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  labs(y = "", x = "Predictors")+
  scale_x_continuous(labels = unique(cvarselp$solution_terms), breaks = unique(cvarselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal")


```

```{r}
# proj_cau_cv <- project(cvs_cau,  solution_terms = c(1, 14))
# 
# proj_samples_cau <- as.matrix(proj_cau_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(1:8), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rel_"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_cau,"./saves/proj_cvs_cau.rds")

proj_samples_cau <- readRDS("./saves/proj_cvs_cau.rds")

pcpro <- ggplot(proj_samples_cau, aes(x = value, y = pred, fill = species)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(factor~., scales = "free")+
  labs(y = "", x = "Estimate")+
  scale_fill_ptol()+
  guides(fill = F)+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```

```{r cselp, fig.height=3.5, fig.cap = "Predictor selection for causal inference. A) Elpd and RMSE values for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel.", out.width="100%"}
ggarrange(pcsel, pcpro, labels = c("A","B"), widths = c(1.5,1))
```

Figure \@ref(fig:cselp) visualizes the results. Out of the 13 predictor variables we analysed, we selected only `group` to be relevant in addition to the random intercept term. When inspecting the projected posterior distribution for `group`, we saw substantial differences between the groups: Orangutans and the b-chimpanzee group performed best, followed by bonobos and finally the gorillas and the a-chimpanzee group (see Figure \@ref(fig:cselp)B). 

#### Inference by exclusion

```{r, warning = F}
# cvs_inf <- cv_varsel(refM_inf, 
#                      search_terms = s_terms, cv_method = "LOO", method = "forward", 
#                      seed = 2020)
# 
# saveRDS(cvs_inf,"./saves/cvs_inf.rds")
# 
# cvs_inf <- readRDS("./saves/cvs_inf.rds")
```

```{r}
# ivarselp <- summary(cvs_inf, stats = c('elpd', 'rmse'))%>%
#   tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(solution_terms = factor(ifelse(is.na(solution_terms),"none",solution_terms)),
#          select = ifelse(solution_terms  %in% c("(1 | subject)", "time_in_leipzig", "group", "age"), "yes", "no"))
# 
# saveRDS(ivarselp, "./saves/cvs_inf_summary.rds")

ivarselp <- readRDS("./saves/cvs_inf_summary.rds")

pisel <- ivarselp%>%
ggplot(aes(x = size, y = value))+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
    labs(y = "", x = "Predictors")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  scale_x_continuous(labels = unique(ivarselp$solution_terms), breaks = unique(ivarselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal")
```

```{r}
# proj_inf_cv <- project(cvs_inf,  solution_terms = c(1, 2, 3))
# 
# proj_samples_inf <- as.matrix(proj_inf_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(1:9), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", "other predictors"),
#          pred = str_remove(pred,"group"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_inf,"./saves/proj_cvs_inf.rds")

proj_samples_inf <- readRDS("./saves/proj_cvs_inf.rds")

pipro <- ggplot(proj_samples_inf, aes(x = value, y = pred, fill = species)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(.~factor, scales = "free", ncol = 1)+
  scale_fill_ptol()+
    labs(y = "", x = "Estimate")+
  guides(fill = F)+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
```

```{r iselp, fig.height=3.5, fig.cap = "Predictor selection for inference. A) Elpd and RMSE values for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel.", out.width="100%"}
ggarrange(pisel, pipro, labels = c("A","B"), widths = c(1.5,1))
```

Figure \@ref(fig:iselp) visualizes the results. For inference by exclusion, we selected `time_in_leipzig`, `group`, and `age` as relevant predictors in addition to the random intercept term. All three predictors capture stable individual characteristics. 

Figure \@ref(fig:iselp)B shows the projected posterior distributions for the predictors and suggests that the longer apes lived in Leipzig, the better their performance was. The differences between groups were such that the two chimpanzee groups together with the gorillas performed on a higher level compared to the bonobos and orangutans. With respect to `age`, we found that performance decreased with age.

#### Gaze Following

```{r, warning = F}
# cvs_gaze <- cv_varsel(refM_gaze, 
#                       search_terms = s_terms_gaze, cv_method = "LOO", method = "forward", 
#                       seed = 2020)
# 
# saveRDS(cvs_gaze,"./saves/cvs_gaze.rds")
# 
# cvs_gaze <- readRDS("./saves/cvs_gaze.rds")
```

```{r}
# gvarselp <- summary(cvs_gaze, stats = c('elpd', 'rmse'))%>%
#   tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(solution_terms = factor(ifelse(is.na(solution_terms),"none",solution_terms)),
#          select = ifelse(solution_terms  %in% c("(1 | subject)", "group", "rearing", "time_outdoors", "age","sociality","sex"), "yes", "no"))
# 
# saveRDS(gvarselp, "./saves/cvs_gaze_summary.rds")

gvarselp <- readRDS("./saves/cvs_gaze_summary.rds")

pgsel <- gvarselp%>%
ggplot(aes(x = size, y = value))+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
      labs(y = "", x = "Predictors")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  scale_x_continuous(labels = unique(gvarselp$solution_terms), breaks = unique(gvarselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal")
```

```{r}
# proj_gaze_cv <- project(cvs_gaze,  solution_terms = c(1:6))
# 
# proj_samples_gaze <- as.matrix(proj_gaze_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(1:14), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", ifelse(grepl("rearing", pred), "rearing",ifelse(grepl("sex", pred), "sex","other predictors"))),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rearing"),
#          pred = str_remove(pred,"sex"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_gaze,"./saves/proj_cvs_gaze.rds")

proj_samples_gaze <- readRDS("./saves/proj_cvs_gaze.rds")

pgpro <- ggplot(proj_samples_gaze, aes(x = value, y = pred, fill = species)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(.~factor, scales = "free", ncol = 4)+
  scale_fill_ptol()+
    labs(y = "", x = "Estimate")+
  scale_x_continuous(guide = guide_axis(n.dodge = 2))+
  guides(fill = F)+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
  
```

```{r gselp, fig.height=6, fig.cap = "Predictor selection for gaze following. A) Elpd and RMSE values for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel model.", out.width="100%"}
ggarrange(pgsel, pgpro, labels = c("A","B"), nrow = 2, heights = c(1.5,1))
```

Figure \@ref(fig:gselp) visualizes the results. Gaze following had the most selected predictors of all tasks. In addition to the random intercept term, we selected, `group`, `rearing`, `time_outdoors`, `age`, `sociality`, and `sex`. Again, most of these predictors were stable individual characteristics, with the exception of `time_outdoors` and `sociality`.

Groups differed in that a-chimpanzees were most likely to follow gaze, followed by b-chimpanzees and bonobos. Gorillas and orangutans were the least likely to follow the experimenter's gaze. Mother-reared individuals outperformed hand-reared individuals (including those with an unknown rearing history). The more time individuals spent outdoors, the more likely they were to follow gaze. Also, the probability to follow gaze increased with age. Individuals with a lower sociality index had higher rates of gaze following. Finally, females outperformed males. Figure \@ref(fig:gselp)B visualizes these results.

#### Quantity

```{r, warning = F}
# cvs_quant <- cv_varsel(refM_quant, 
#                        search_terms = s_terms, cv_method = "LOO", method = "forward", 
#                        seed = 2020)
# 
# saveRDS(cvs_quant,"./saves/cvs_quant.rds")
# 
# cvs_quant <- readRDS("./saves/cvs_quant.rds")
```

```{r}
# qvarselp <- summary(cvs_quant, stats = c('elpd', 'rmse'))%>%
#   tibble()%>%
#   pivot_longer(cols = c(elpd,rmse), values_to = "value", names_to = "stat")%>%
#   pivot_longer(cols = c(elpd.se,rmse.se), values_to = "se", names_to = "se_term")%>%
#   rowwise()%>%
#   filter(grepl(stat,se_term))%>%
#   select(-se_term)%>%
#   mutate(solution_terms = factor(ifelse(is.na(solution_terms),"none",solution_terms)),
#          select = ifelse(solution_terms  %in% c("(1 | subject)", "time_in_leipzig", "rearing", "group"), "yes", "no"))
# 
# saveRDS(qvarselp, "./saves/cvs_quant_summary.rds")

qvarselp <- readRDS("./saves/cvs_quant_summary.rds")

pqsel <- qvarselp%>%
ggplot(aes(x = size, y = value))+
  geom_line(col = "black", alpha = .5)+
  geom_pointrange(aes(ymin = value - se, ymax = value + se, col = select), size = 0.5, pch = 4)+
  facet_grid(stat~., scale = "free_y")+
        labs(y = "", x = "Predictors")+
  scale_color_manual(name = "relevant", values = c("black", "firebrick"))+
  scale_x_continuous(labels = unique(qvarselp$solution_terms), breaks = unique(qvarselp$size))+
  theme_few()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        legend.position = c(0.5,0.9), legend.direction = "horizontal")
```

```{r}
# proj_quant_cv <- project(cvs_quant,  solution_terms = c(1:3))
# 
# proj_samples_quant <- as.matrix(proj_quant_cv)%>%
#   as_tibble()%>%
#   pivot_longer(cols =c(1:10), names_to = "pred", values_to = "value")%>%
#   filter(!pred%in%c("test_tp", "sigma", "Intercept"))%>%
#   mutate(factor = ifelse(grepl("group", pred), "group", ifelse(grepl("rearing", pred), "rearing",ifelse(grepl("sex", pred), "sex","other predictors"))),
#          pred = str_remove(pred,"group"),
#          pred = str_remove(pred,"rearing"),
#          pred = str_remove(pred,"sex"),
#          species = ifelse(factor == "group", pred,factor))
# 
# saveRDS(proj_samples_quant,"./saves/proj_cvs_quant.rds")

proj_samples_quant <- readRDS("./saves/proj_cvs_quant.rds")

pqpro <- ggplot(proj_samples_quant, aes(x = value, y = pred, fill = species)) +
  geom_vline(xintercept = 0, lty = 2, alpha = .75)+
  geom_density_ridges(alpha = .5, scale = .7) +
  facet_wrap(.~factor, scales = "free", ncol = 3)+
  scale_fill_ptol()+
    labs(y = "", x = "Estimate")+
  guides(fill = F)+
  theme_minimal()+
  theme(panel.border = element_rect(color = "black",fill = NA,size = 1))
  
```

```{r qselp, fig.height=6, fig.cap = "Predictor selection for quantity. A) Elpd and RMSE values for predictors, ordered by importance (left to right) according to the cross-validated projection prediction model. Note that the random intercept term was forced to be the last one to be included. B) Projections for the selected predictors based on the submodel model.", out.width="100%"}
ggarrange(pqsel, pqpro, labels = c("A","B"), nrow = 2, heights = c(1.5,1))
```

Figure \@ref(fig:qselp) visualizes the results. For quantity, we selected three predictors in addition to the random intercept term:`time_in_leipzig`, `rearing`, and `group`. All of these predictors were stable individual characteristics.

The longer individuals had lived in Leipzig, the better they performed in the task. Group differences were such that b-chimpanzees performed best, followed by bonobos and gorillas. A-Chimpanzees performed slightly worse, but still better than the orangutans. Once again, mother reared individuals outperformed those who were hand-reared or whose rearing history was unknown. Figure \@ref(fig:qselp)B visualizes these results.

#### Summary

The most obvious result was that the random intercept term `(1 | subject)` was -- by far -- the predictor that improved the model fit the most. This suggests that a large portion of the variance is explained by stable individual characteristics that we did not capture in our predictors. Most likely, these are the outcomes of idiosyncratic developmental processes or genetic pre-dispositions, which operate on a much longer time-scale than what we captured in our study. 

Second, we saw that most of the relevant predictors came from the group of stable individual characteristics. This aligns well with the SEM results, in which we saw that most of the variance in performance could be traced back to stable trait differences between individuals. Following this reasoning, there was very little *systematic* variation between time points, and thus not much the time-varying predictors could account for. In line with this interpretation, we selected time point specific predictors only for gaze following, the task with the highest occasion specificity estimate according to the LSTM.

The predictor selected most often was `group`. Differences between groups were, however, variable. The b-chimpanzee group tended to perform best across tasks, but the ranking of the other groups (including the other chimpanzee group) changed from task to task. This speaks against clear species differences in general cognitive performance. Again, the most likely explanation for group differences is an interaction between species specific dispositions and individual- / group-level developmental processes.

The predictors that were selected more than once influenced performance in a systematic way. Whenever rearing history was selected to be relevant, mother-reared individuals outperformed others. The more time an individual had lived in Leipzig, the better performance was. An exception was `age`, which had a positive estimate for gaze following but a negative one for inference. 

When zooming out, we found no clear ranking of predictors across tasks (see Figure \@ref(fig:selcomp)). It is important to note, however, that for higher ranks, the difference between ranks in the loss statistic is very small and the ordering to some extend arbitrary. But even if we focus only on the five highest-ranked predictors per task, we see a lot of variation across tasks -- with `group` being a notable exception.

```{r}
ro <- bind_rows(qvarselp%>%mutate(task = "quantity"),
          cvarselp%>%mutate(task = "causality"),
          ivarselp%>%mutate(task = "inference"),
          gvarselp%>%mutate(task = "gaze_following"),
)%>%
  filter(stat == "elpd", 
         solution_terms != "none",
         solution_terms != "day2",
         solution_terms != "(1 | subject)")%>%
  select(size, solution_terms, task, select)%>%
  group_by(solution_terms)%>%
  mutate(sum = sum(size))

# ggplot(ro,aes(x = task, y = size, col = solution_terms))+
#   geom_line(aes(group = solution_terms))+
#   geom_point(aes(size = select))+
#   theme_minimal()+
#   labs(x = "", y = "Predictor rank based on PPI model")+
#   scale_y_reverse(breaks = c(1:15))+
#   scale_x_discrete(expand=c(0,0.7)) +
#   guides(col = F, size = F)+
#   geom_dl(aes(label = solution_terms),method = list(dl.trans(x = x + 0.5), "last.points", cex = 0.7))+
#   scale_color_viridis_d()
# 
# ggplot(ro,aes(x = fct_reorder(solution_terms,-sum), y = size, fill = task))+
#   #geom_line(aes(group = task))+
#   geom_bar(stat = "identity", position = position_dodge(), alpha = .75)+
#   theme_minimal()+
#   labs(x = "", y = "Predictor rank based on PPI model")+
#   coord_flip()+
#   scale_fill_viridis_d()


pro <- ggplot(ro,aes(x = fct_reorder(solution_terms,sum), y = size, col = task))+
  #geom_line(aes(group = task))+
  geom_line(aes(group = task), alpha = .5)+
  geom_point(aes(pch = select), size = 3)+
  scale_shape_manual(values=c(1,4))+
  theme_minimal()+
  labs(x = "Predictor (ordered by average rank)", y = "Rank based on PPI model")+
  scale_color_viridis_d()+
  guides(shape = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
pco <- ro%>%
  ungroup()%>%
  select(size, solution_terms,task)%>%
  pivot_wider(names_from = "task", values_from = "size")%>%
  select(-solution_terms)%>%
  corrr::correlate(method = "spearman", quiet = T)%>%
  gather(task, cor, -term)%>%
  mutate(cor = replace(cor, duplicated(cor), NA))%>%
  drop_na(cor)%>%
  mutate(cor = round(cor,2))%>%
  ggplot(aes(y = task, x = term, fill = cor, label = cor))+
  geom_tile(color = "white")+
  labs(x = "", y = "")+
  scale_fill_gradient2(low = "#CC6677", high = "#117733", mid = "#FFE066", 
  midpoint = 0, limit = c(-1,1), space = "Lab", name="Correlation")+
  geom_text()+
  scale_x_discrete(limits = c("gaze_following","inference","causality"))+
  scale_y_discrete(limits = c("quantity","causality","inference"))+
  theme_few()+
  theme(legend.position = "right", legend.direction = "vertical", legend.title = element_blank())+
  coord_fixed()
  
```

```{r selcomp, fig.height=3, fig.cap = "Ranking for each predictor and task. Crosses denote selected predictors.", out.width="100%"}
pro
```

```{r cselcomp, fig.height=3, fig.cap = "Correlations of predictor rankings among the four tasks.", out.width="100%"}
#pco
```

### Phase 2

### Comparison between phases

# Summary

# References

<div id="refs"></div>

# Appendix

## SEM Simulations

```{r}
sim <- read.table("../../models and documentation Jana/Simulation/Simulation_results_long.txt",sep="\t",header=TRUE)%>%
  mutate(model = as.character(as.numeric(factor(files))),
         parameter = factor(kind))

psim <- sim%>%
  select(model,parameter, cover_95, peb,mse,bias,seb)%>%
  pivot_longer(cols = c(cover_95, peb,mse,bias,seb), values_to = "value", names_to = "measure")%>%
  mutate(uline = ifelse(measure == "cover_95", .98, 
                        ifelse(measure == "peb" | measure == "seb", .1, NA)),
         lline = ifelse(measure == "cover_95", .91,NA))%>%
  mutate(measure = recode(measure, 
                          cover_95 = "95%-Coverage",
                          peb = "Rel. Parameter\n Estimation Bias",
                          mse = "Mean Squared\n Error",
                          bias = "Bias",
                          seb = "Rel. Standard\n Error Bias"))
```

### Simulation setup

Data were generated and estimated using MPlus 8.4. Data-generating values are based on the real-data application of the models to the available subset of the data at the time of conducting the simulation study. That is, data were simulated for 40 individuals (N) observed across 9 or 12 measurement occasions, with 5 or 7 observed categories per indicator. 1000 replications were simulated. Data estimation took place using the MPlus default priors. In case of LST models for one construct, default priors were compared with IG(0.001, 0.001) priors set on all variance parameters (model did not include latent covariances). Two chains with a minimum of 5,000 iterations per chain and a thinning factor of 10 was applied (i.e. at last 50,000 iterations of which only every 10th was used for constructing the posterior distribution). Convergence was assumed and estimation stopped when the PSR fell below 1.05 for the first time after the minimum number of iterations was reached.

### Simulation results

In the following, the 95% coverage rate, the Relative Parameter Estimation Bias (deviation between average estimate and population parameter divided by the population parameter), the Mean Squared Error, absolute bias, as well as Relative Standard Error Bias are displayed for every simulated model (Figure \@ref(fig:sim1) - \@ref(fig:sim4)). Relative parameter and standard error biases below 0.1 (that is $< 10 \%$) are considered acceptable. 

Parameters in the latent state models for one construct are estimated accurately, with relative biases below a cutoff of 10% bias and good coverage rates, irrespective of simulating 7 or 5 observed ordered categories for the observed indicators. 

Latent state-trait models for one construct with latent state residual variances fixed across time show good estimation performance, with both default or adapted inverse gamma priors. When freely estimating latent state residual variances across time points (i.e., no restrictions on variances), model parameters are not estimated accurately under the simulated sample sizes, irrespective of the prior choice.

Latent state-trait models for a combination of two constructs with latent variances and covariances of the state residual variances restricted to equality across time points work well. Models with freely estimated variances and covariances do not show good estimation performance. The same holds for the latent state model with two constructs and freely estimated variances and covariances.

In conclusion, latent state models for one construct (freely estimated variances) as well as latent state-trait models for one or two constructs with state residual variances restricted across time exhibit good estimation performance (low biases, high coverage) and application under the simulated samples size should be feasible in practice.

```{r sim1, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State (LS) model including one construct with freely estimated latent State variances and covariances, spanning 9 measurement time points. Ordinal indicators were simulated with 5 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "2")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State models: One construct",
          subtitle = "Freely varying state variances and covariances across time points. 5 ordered categories")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))
```

```{r sim3, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State (LS) model including one construct with freely estimated latent State variances and covariances, spanning 9 measurement time points. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "3")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State models: One construct",
          subtitle = "Freely varying state variances and covariances across time points. 7 ordered categories")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```
```{r sim7, fig.height=3.5, fig.cap = " Results of the simulation study for the Latent State-Trait (LST) model including one construct with latent state residual variances fixed to be equal across time, spanning 9 measurement time points. MPlus default priors. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "7")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State-Trait models: One construct",
          subtitle = "Fixed state residual variances across time points with default priors")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```

```{r sim6, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State-Trait (LST) model including one construct with latent state residual variances fixed to be equal across time, spanning 9 measurement time points. Inverse gamma priors IG(0.001,0.001) for all variances. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "6")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State-Trait models: One construct",
          subtitle = "Fixed state residual variances across time points with inverse gamma priors")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```

```{r sim8, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State-Trait (LST) model including one construct with latent state residual variances freely estimates across time, spanning 9 measurement time points. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "8")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State-Trait models: One construct",
          subtitle = "Free state residual variances across time points with default priors")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```

```{r sim9, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State-Trait (LST) model including one construct with latent state residual variances freely estimates across time, spanning 9 measurement time points. Inverse gamma priors IG(0.001,0.001) for all variances. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "9")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State-Trait models: One construct",
          subtitle = "Free state residual variances across time points with inverse gamma priors")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```


```{r sim2, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State model including two constructs with latent state variances freely estimates across time, spanning 9 measurement time points. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "1")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State models: Two constructs",
          subtitle = "Free state variances and covariances across time points and default priors")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```

```{r sim5, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State-Trait (LST) model including two constructs with free latent state residual variances across time, spanning 9 measurement time points. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "5")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State-Trait models: Two constructs",
          subtitle = "Free state residual variances and covariances across time points")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```

```{r sim4, fig.height=3.5, fig.cap = "Results of the simulation study for the Latent State-Trait (LST) model including two constructs with fixed latent state residual variances across time, spanning 9 measurement time points. Ordinal indicators were simulated with 7 ordered categories. Boxplots display the distribution of the respective statistic across different parameters of the same parameter type.", out.width="100%"}
psim %>%
  filter(model == "4")%>%
  ggplot(.,aes(x=parameter,y=value, fill = parameter)) + 
  geom_boxplot(outlier.colour="red", #outlier.shape=8,
               outlier.size=0.2,notch=F, alpha = .75)+
  geom_hline(aes(yintercept = uline), col = "red", lty = 2)+
  geom_hline(aes(yintercept = lline), col = "red", lty = 2)+
  facet_wrap_custom(measure~., scales = "free", nrow = 1, scale_overrides = list(
    scale_override(1, scale_y_continuous(limits = c(0.8, 1))),
    scale_override(2, scale_y_continuous(limits = c(-1,1))),
    scale_override(3, scale_y_continuous(limits = c(0,2))),
    scale_override(4, scale_y_continuous(limits = c(0,0.5))),
    scale_override(5, scale_y_continuous(limits = c(0,1)))
  ))+
  ggtitle("Latent State-Trait models: Two constructs",
          subtitle = "Fixed state residual variances and covariances across time points")+
  scale_fill_manual(values=c("#003366", "#006699","#3399CC","#99CCFF","#CCFFFF"))+
  guides(fill = "none")+
  labs(x = "", y = "")+
  theme_minimal()+
  theme(axis.text.x = element_text(angle=45,hjust = 1))

```




